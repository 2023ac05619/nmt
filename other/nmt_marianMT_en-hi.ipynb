{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c49c30",
   "metadata": {},
   "source": [
    "## Fine-tune MarianMT model for English-Hindi translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "442fec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Fine-Tuning with MarianMT ---\n",
      "Loading Helsinki-NLP/opus-mt-en-hi...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c7d66eed734c178bd06b7a4754998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69d5b8481a44f6797628c5cf3e867e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653a1e5e19c34c49bd06344cdd25de33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738c2106a1064d34aa5892486960893f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724ff33ae0a44396a2868f7fe5ed3478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e4236978f34bf4bea750e2370b5838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16491dcb448546ebbe91915a83e30b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading samanantar dataset...\n",
      "Found 5000 valid translation pairs\n",
      "Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5822fb1c324e59ae9147354c4bc6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d672823e88584fc3918e873598ec435b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.168100</td>\n",
       "      <td>3.757415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.692200</td>\n",
       "      <td>3.588024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.359100</td>\n",
       "      <td>3.524310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.313500</td>\n",
       "      <td>3.505749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to results/marian_en_hi_finetuned\n",
      "\n",
      "--- Starting Evaluation ---\n",
      "Loading translator from: results/marian_en_hi_finetuned\n",
      "Detected model type: marian\n",
      "Translator ready!\n",
      "Evaluating translations...\n",
      " 1. 'Hello'\n",
      "    → सलाम\n",
      " 2. 'How are you?'\n",
      "    → आप कैसे हैं?\n",
      " 3. 'Good morning'\n",
      "    → सुप्रभात\n",
      " 4. 'Thank you'\n",
      "    → धन्यवाद\n",
      " 5. 'I am fine'\n",
      "    → मैं ठीक हूं\n",
      " 6. 'What is your name?'\n",
      "    → आपका GroupWise कूटशब्द क्या है?\n",
      " 7. 'Nice to meet you'\n",
      "    → आपसे मिलकर अच्छा लगा\n",
      " 8. 'How much does this cost?'\n",
      "    → यह खर्च कितना है?\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS\n",
      "================================================================================\n",
      "                  Source           Reference                      Prediction   BLEU  METEOR\n",
      "                   Hello              नमस्ते                            सलाम   0.00    0.00\n",
      "            How are you?        आप कैसे हैं?                    आप कैसे हैं? 100.00   99.22\n",
      "            Good morning            सुप्रभात                        सुप्रभात  17.78   50.00\n",
      "               Thank you             धन्यवाद                         धन्यवाद  17.78   50.00\n",
      "               I am fine         मैं ठीक हूं                     मैं ठीक हूं  56.23   98.15\n",
      "      What is your name?   आपका नाम क्या है? आपका GroupWise कूटशब्द क्या है?  21.71   73.53\n",
      "        Nice to meet you आपसे मिलकर खुशी हुई            आपसे मिलकर अच्छा लगा  16.99   46.88\n",
      "How much does this cost? इसकी कीमत कितनी है?               यह खर्च कितना है?  11.36   37.50\n",
      "\n",
      "Average BLEU: 34.55\n",
      "Average METEOR: 65.04\n",
      "Success rate: 7/8 (87.5%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def prepare_translation_data():\n",
    "    \"\"\"Prepare translation data from samanantar dataset.\"\"\"\n",
    "    print(\"Loading samanantar dataset...\")\n",
    "    dataset = load_dataset(\"ai4bharat/samanantar\", \"hi\", split='train')\n",
    "    \n",
    "    # Filter valid pairs and create smaller dataset for testing\n",
    "    valid_pairs = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= 5000:  # Use smaller dataset for debugging\n",
    "            break\n",
    "        if example['src'] and example['tgt'] and len(example['src'].strip()) > 0 and len(example['tgt'].strip()) > 0:\n",
    "            valid_pairs.append({\n",
    "                'english': example['src'].strip(),\n",
    "                'hindi': example['tgt'].strip()\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(valid_pairs)} valid translation pairs\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_idx = int(0.9 * len(valid_pairs))\n",
    "    train_pairs = valid_pairs[:split_idx]\n",
    "    val_pairs = valid_pairs[split_idx:]\n",
    "    \n",
    "    return train_pairs, val_pairs\n",
    "\n",
    "def fine_tune_marian_model():\n",
    "    \"\"\"Fine-tune MarianMT model for English-Hindi translation.\"\"\"\n",
    "    print(\"--- Starting Fine-Tuning with MarianMT ---\")\n",
    "    \n",
    "    # Use Helsinki-NLP's multilingual model as base\n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "    output_dir = \"results/marian_en_hi_finetuned\"\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Prepare data\n",
    "        train_pairs, val_pairs = prepare_translation_data()\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            inputs = examples[\"english\"]\n",
    "            targets = examples[\"hindi\"] \n",
    "            model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "            \n",
    "            # Setup the tokenizer for targets\n",
    "            labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        # Convert to datasets\n",
    "        train_dataset = Dataset.from_list(train_pairs)\n",
    "        val_dataset = Dataset.from_list(val_pairs)\n",
    "        \n",
    "        print(\"Preprocessing datasets...\")\n",
    "        train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            logging_steps=50,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=200,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        print(f\"Saving model to {output_dir}\")\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MarianMT failed: {e}\")\n",
    "        print(\"Falling back to T5 model...\")\n",
    "        return fine_tune_t5_model()\n",
    "\n",
    "def fine_tune_t5_model():\n",
    "    \"\"\"Fallback: Fine-tune T5 model for translation.\"\"\"\n",
    "    from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "    \n",
    "    model_name = \"t5-small\"\n",
    "    output_dir = \"results/t5_en_hi_finetuned\"\n",
    "    \n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_pairs, val_pairs = prepare_translation_data()\n",
    "    \n",
    "    # Create training data with T5 format\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    for pair in train_pairs:\n",
    "        train_data.append({\n",
    "            'input_text': f\"translate English to Hindi: {pair['english']}\",\n",
    "            'target_text': pair['hindi']\n",
    "        })\n",
    "    \n",
    "    for pair in val_pairs:\n",
    "        val_data.append({\n",
    "            'input_text': f\"translate English to Hindi: {pair['english']}\",\n",
    "            'target_text': pair['hindi']\n",
    "        })\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"input_text\"]\n",
    "        targets = examples[\"target_text\"]\n",
    "        model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "        labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Convert to datasets\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    \n",
    "    print(\"Preprocessing datasets...\")\n",
    "    train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "    val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        logging_steps=50,\n",
    "        save_steps=200,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=200,\n",
    "        predict_with_generate=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting T5 training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Saving T5 model to {output_dir}\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "class UniversalTranslator:\n",
    "    \"\"\"Universal translator that works with different model types.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        print(f\"Loading translator from: {model_path}\")\n",
    "        \n",
    "        # Detect model type\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        self.model_type = \"unknown\"\n",
    "        \n",
    "        if os.path.exists(config_path):\n",
    "            import json\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                if \"marian\" in config.get(\"architectures\", [\"\"])[0].lower():\n",
    "                    self.model_type = \"marian\"\n",
    "                elif \"t5\" in config.get(\"architectures\", [\"\"])[0].lower():\n",
    "                    self.model_type = \"t5\"\n",
    "        \n",
    "        print(f\"Detected model type: {self.model_type}\")\n",
    "        \n",
    "        if self.model_type == \"marian\":\n",
    "            self.tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "            self.model = MarianMTModel.from_pretrained(model_path)\n",
    "        else:  # Default to T5\n",
    "            from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "            self.model_type = \"t5\"\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"Translator ready!\")\n",
    "\n",
    "    def translate(self, text, src_lang='en', tgt_lang='hi'):\n",
    "        \"\"\"Translate text based on model type.\"\"\"\n",
    "        if self.model_type == \"marian\":\n",
    "            # MarianMT expects plain text\n",
    "            input_text = text\n",
    "        else:  # T5\n",
    "            input_text = f\"translate English to Hindi: {text}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                length_penalty=0.6,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation.strip()\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self):\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    def calculate_bleu(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(sentence_bleu([ref_tokens], cand_tokens, smoothing_function=self.smoothing) * 100, 2)\n",
    "    \n",
    "    def calculate_meteor(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(meteor_score([ref_tokens], cand_tokens) * 100, 2)\n",
    "\n",
    "def run_evaluation(model_path):\n",
    "    \"\"\"Evaluate the trained model.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    \n",
    "    # Download NLTK data\n",
    "    for corpus in ['punkt', 'wordnet', 'omw-1.4']:\n",
    "        nltk.download(corpus, quiet=True)\n",
    "\n",
    "    translator = UniversalTranslator(model_path)\n",
    "    evaluator = TranslationEvaluator()\n",
    "    \n",
    "    test_cases = [\n",
    "        {'source': 'Hello', 'reference': 'नमस्ते'},\n",
    "        {'source': 'How are you?', 'reference': 'आप कैसे हैं?'},\n",
    "        {'source': 'Good morning', 'reference': 'सुप्रभात'},\n",
    "        {'source': 'Thank you', 'reference': 'धन्यवाद'},\n",
    "        {'source': 'I am fine', 'reference': 'मैं ठीक हूं'},\n",
    "        {'source': 'What is your name?', 'reference': 'आपका नाम क्या है?'},\n",
    "        {'source': 'Nice to meet you', 'reference': 'आपसे मिलकर खुशी हुई'},\n",
    "        {'source': 'How much does this cost?', 'reference': 'इसकी कीमत कितनी है?'},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    print(\"Evaluating translations...\")\n",
    "    \n",
    "    for i, case in enumerate(test_cases, 1):\n",
    "        print(f\"{i:2d}. '{case['source']}'\")\n",
    "        \n",
    "        try:\n",
    "            prediction = translator.translate(case['source'])\n",
    "            bleu = evaluator.calculate_bleu(case['reference'], prediction)\n",
    "            meteor = evaluator.calculate_meteor(case['reference'], prediction)\n",
    "            \n",
    "            results.append({\n",
    "                'Source': case['source'],\n",
    "                'Reference': case['reference'], \n",
    "                'Prediction': prediction,\n",
    "                'BLEU': bleu,\n",
    "                'METEOR': meteor\n",
    "            })\n",
    "            \n",
    "            print(f\"    → {prediction}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    → ERROR: {str(e)}\")\n",
    "            results.append({\n",
    "                'Source': case['source'],\n",
    "                'Reference': case['reference'],\n",
    "                'Prediction': f\"ERROR: {str(e)}\",\n",
    "                'BLEU': 0.0,\n",
    "                'METEOR': 0.0\n",
    "            })\n",
    "    \n",
    "    # Results summary\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df.to_string(index=False, max_colwidth=40))\n",
    "    \n",
    "    valid_results = df[df['BLEU'] > 0]\n",
    "    if len(valid_results) > 0:\n",
    "        avg_bleu = valid_results['BLEU'].mean()\n",
    "        avg_meteor = valid_results['METEOR'].mean()\n",
    "        print(f\"\\nAverage BLEU: {avg_bleu:.2f}\")\n",
    "        print(f\"Average METEOR: {avg_meteor:.2f}\")\n",
    "        print(f\"Success rate: {len(valid_results)}/{len(results)} ({100*len(valid_results)/len(results):.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\nNo successful translations generated.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    # Train model (try MarianMT first, fallback to T5)\n",
    "    model_path = fine_tune_marian_model()\n",
    "    \n",
    "    # Evaluate model\n",
    "    run_evaluation(model_path)\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65cba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing mBART translator...\n",
      "Loading mBART translator from: results/mbart_en_hi_bidirectional\n",
      "mBART translator ready!\n",
      "Fine-tuned mBART translator initialized.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.202.209:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:02] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:03] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:05] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:16] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:16] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:23] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:23] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:26] \"POST /api/translate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import time\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Global translator instance\n",
    "translator = None\n",
    "LANGUAGES = {'en': 'English', 'hi': 'Hindi'}\n",
    "\n",
    "def initialize_translator():\n",
    "    global translator\n",
    "    print(\"Initializing mBART translator...\")\n",
    "    try:\n",
    "        # Try fine-tuned model first\n",
    "        model_path = \"results/mbart_en_hi_bidirectional\"\n",
    "        translator = MBartTranslator(model_path)\n",
    "        print(\"Fine-tuned mBART translator initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fine-tuned model not found: {e}\")\n",
    "        print(\"Using pre-trained mBART model...\")\n",
    "        try:\n",
    "            # Fallback to pre-trained mBART\n",
    "            class PretrainedMBart:\n",
    "                def __init__(self):\n",
    "                    from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "                    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "                    self.tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "                    self.model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "                    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    self.model.to(self.device)\n",
    "                    self.model.eval()\n",
    "                    \n",
    "                def translate(self, text, src_lang='en', tgt_lang='hi'):\n",
    "                    lang_codes = {'en': 'en_XX', 'hi': 'hi_IN'}\n",
    "                    src_code = lang_codes[src_lang]\n",
    "                    tgt_code = lang_codes[tgt_lang]\n",
    "                    \n",
    "                    self.tokenizer.src_lang = src_code\n",
    "                    inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    generated_tokens = self.model.generate(\n",
    "                        **inputs,\n",
    "                        forced_bos_token_id=self.tokenizer.lang_code_to_id[tgt_code],\n",
    "                        max_length=128,\n",
    "                        num_beams=5,\n",
    "                    )\n",
    "                    \n",
    "                    return self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0].strip()\n",
    "            \n",
    "            translator = PretrainedMBart()\n",
    "            print(\"Pre-trained mBART translator initialized.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to initialize any translator: {e2}\")\n",
    "            translator = None\n",
    "\n",
    "@app.route('/api/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'translator_ready': translator is not None,\n",
    "        'supported_languages': LANGUAGES\n",
    "    })\n",
    "\n",
    "@app.route('/api/translate', methods=['POST'])\n",
    "def translate_text():\n",
    "    try:\n",
    "        if translator is None:\n",
    "            return jsonify({'error': 'Translator not initialized. Please check model path.'}), 503\n",
    "        \n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        text = data.get('text', '').strip()\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "\n",
    "        if not text:\n",
    "            return jsonify({'error': 'No text provided for translation.'}), 400\n",
    "        if src_lang not in LANGUAGES or tgt_lang not in LANGUAGES:\n",
    "            return jsonify({'error': 'Unsupported language selected.'}), 400\n",
    "        if src_lang == tgt_lang:\n",
    "            return jsonify({'error': 'Source and target languages are the same.'}), 400\n",
    "\n",
    "        start_time = time.time()\n",
    "        translation = translator.translate(text, src_lang, tgt_lang)\n",
    "        end_time = time.time()\n",
    "\n",
    "        return jsonify({\n",
    "            'source_text': text,\n",
    "            'source_language': src_lang,\n",
    "            'target_language': tgt_lang,\n",
    "            'translation': translation,\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/batch-translate', methods=['POST'])\n",
    "def batch_translate():\n",
    "    try:\n",
    "        if translator is None:\n",
    "            return jsonify({'error': 'Translator not initialized.'}), 503\n",
    "            \n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        texts = data.get('texts', [])\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "        \n",
    "        if not texts or not isinstance(texts, list):\n",
    "            return jsonify({'error': 'No texts array provided.'}), 400\n",
    "        if len(texts) > 100:\n",
    "            return jsonify({'error': 'Maximum 100 texts allowed per batch.'}), 400\n",
    "\n",
    "        start_time = time.time()\n",
    "        translations = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if isinstance(text, str) and text.strip():\n",
    "                try:\n",
    "                    translation = translator.translate(text.strip(), src_lang, tgt_lang)\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': translation,\n",
    "                        'success': True\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': None,\n",
    "                        'success': False,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "            else:\n",
    "                translations.append({\n",
    "                    'source': text,\n",
    "                    'translation': None,\n",
    "                    'success': False,\n",
    "                    'error': 'Invalid text format'\n",
    "                })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        return jsonify({\n",
    "            'translations': translations,\n",
    "            'total_count': len(translations),\n",
    "            'success_count': sum(1 for t in translations if t['success']),\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Batch translation error: {e}\")\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/languages', methods=['GET'])\n",
    "def get_languages():\n",
    "    return jsonify({\n",
    "        'supported_languages': LANGUAGES,\n",
    "        'translation_pairs': [\n",
    "            {'source': 'en', 'target': 'hi'},\n",
    "            {'source': 'hi', 'target': 'en'}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "def run_flask_app():\n",
    "    initialize_translator()\n",
    "    app.run(debug=False, use_reloader=False, host='0.0.0.0', port=5000, threaded=True)\n",
    "\n",
    "def start_api_server():\n",
    "    thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "    thread.start()\n",
    "    return thread\n",
    "\n",
    "# For Jupyter notebook usage\n",
    "def start_translation_api():\n",
    "    print(\"Starting translation API server...\")\n",
    "    thread = start_api_server()\n",
    "    print(\"API server started on http://localhost:5000\")\n",
    "    print(\"\\nAvailable endpoints:\")\n",
    "    print(\"  GET  /api/health - Check server status\")\n",
    "    print(\"  GET  /api/languages - Get supported languages\")\n",
    "    print(\"  POST /api/translate - Translate single text\")\n",
    "    print(\"  POST /api/batch-translate - Translate multiple texts\")\n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"curl -X POST http://localhost:5000/api/translate \\\\\")\n",
    "    print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "    print(\"  -d '{\\\"text\\\": \\\"Hello\\\", \\\"src_lang\\\": \\\"en\\\", \\\"tgt_lang\\\": \\\"hi\\\"}'\")\n",
    "    return thread\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_flask_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt_rerun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
