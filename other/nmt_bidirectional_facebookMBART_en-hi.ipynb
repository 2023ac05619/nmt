{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c49c30",
   "metadata": {},
   "source": [
    "## Fine-tune MarianMT model for English-Hindi translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tune mBART for bidirectional EN-HI translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a9cdfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting mBART fine-tuning...\n",
      "--- Fine-tuning mBART-50 ---\n",
      "Loading facebook/mbart-large-50-many-to-many-mmt...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92714ef29ff44298e1dfa7d62ae6975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f81aa6ac164556822788c0253e7a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31023dfd935b4c138ec010d8b768a66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88bcb6441a541739d4b1dc9e2d3c557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e226ad3f2394635b57ef98bd87a393e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8ef0bf29de431bacdf42b75c146efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading samanantar dataset for mBART...\n",
      "Prepared 18000 train, 2000 val samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993c8b06975547f1812f7458225ac355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32197efab51a4cb492f18662ed4771e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2bb5818efc4838a711ce2613df8496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3990dea4e6450caec7ea44d7021fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset sizes: train=18000, val=2000\n",
      "Starting mBART training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3375/3375 15:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.957300</td>\n",
       "      <td>1.967723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.949900</td>\n",
       "      <td>1.899083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.782300</td>\n",
       "      <td>1.877854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.700300</td>\n",
       "      <td>1.861613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.589000</td>\n",
       "      <td>1.865013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.552600</td>\n",
       "      <td>1.858425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving mBART model to results/mbart_en_hi_bidirectional\n",
      "Loading mBART translator from: results/mbart_en_hi_bidirectional\n",
      "mBART translator ready!\n",
      "Using fine-tuned mBART model\n",
      "\n",
      "--- Starting Evaluation ---\n",
      "Evaluating translations...\n",
      " 1. 'Hello' (en->hi)\n",
      "    → नमस्ते\n",
      " 2. 'How are you?' (en->hi)\n",
      "    → आप कैसे हैं?\n",
      " 3. 'Good morning' (en->hi)\n",
      "    → नमस्ते\n",
      " 4. 'Thank you' (en->hi)\n",
      "    → धन्यवाद\n",
      " 5. 'I am fine' (en->hi)\n",
      "    → मैं ठीक हूं।\n",
      " 6. 'नमस्ते' (hi->en)\n",
      "    → goodbye\n",
      " 7. 'आप कैसे हैं?' (hi->en)\n",
      "    → How are you?\n",
      " 8. 'धन्यवाद' (hi->en)\n",
      "    → thank you\n",
      " 9. 'मैं ठीक हूं' (hi->en)\n",
      "    → I'm fine.\n",
      "10. 'सुप्रभात' (hi->en)\n",
      "    → sunrise\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS\n",
      "================================================================================\n",
      "      Source    Reference   Prediction   BLEU  METEOR\n",
      "       Hello       नमस्ते       नमस्ते  17.78   50.00\n",
      "How are you? आप कैसे हैं? आप कैसे हैं? 100.00   99.22\n",
      "Good morning     सुप्रभात       नमस्ते   0.00    0.00\n",
      "   Thank you      धन्यवाद      धन्यवाद  17.78   50.00\n",
      "   I am fine  मैं ठीक हूं मैं ठीक हूं।  24.03   62.50\n",
      "      नमस्ते        Hello      goodbye   0.00    0.00\n",
      "आप कैसे हैं? How are you? How are you? 100.00   99.22\n",
      "     धन्यवाद    Thank you    thank you  31.62   93.75\n",
      " मैं ठीक हूं    I am fine    I'm fine.   9.55   32.26\n",
      "    सुप्रभात Good morning      sunrise   0.00    0.00\n",
      "\n",
      "Average BLEU: 42.97\n",
      "Average METEOR: 69.56\n",
      "Success rate: 7/10 (70.0%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def prepare_mbart_data():\n",
    "    \"\"\"Prepare data for mBART fine-tuning.\"\"\"\n",
    "    print(\"Loading samanantar dataset for mBART...\")\n",
    "    dataset = load_dataset(\"ai4bharat/samanantar\", \"hi\", split='train')\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= 10000:\n",
    "            break\n",
    "        if example['src'] and example['tgt']:\n",
    "            # EN->HI\n",
    "            train_data.append({\n",
    "                'source': example['src'].strip(),\n",
    "                'target': example['tgt'].strip(),\n",
    "                'src_lang': 'en_XX',\n",
    "                'tgt_lang': 'hi_IN'\n",
    "            })\n",
    "            # HI->EN\n",
    "            train_data.append({\n",
    "                'source': example['tgt'].strip(),\n",
    "                'target': example['src'].strip(),\n",
    "                'src_lang': 'hi_IN',\n",
    "                'tgt_lang': 'en_XX'\n",
    "            })\n",
    "    \n",
    "    # Shuffle and split\n",
    "    import random\n",
    "    random.seed(42)\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    split_idx = int(0.9 * len(train_data))\n",
    "    train_set = train_data[:split_idx]\n",
    "    val_set = train_data[split_idx:]\n",
    "    \n",
    "    print(f\"Prepared {len(train_set)} train, {len(val_set)} val samples\")\n",
    "    return train_set, val_set\n",
    "\n",
    "def fine_tune_mbart():\n",
    "    \"\"\"Fine-tune mBART for bidirectional EN-HI translation.\"\"\"\n",
    "    print(\"--- Fine-tuning mBART-50 ---\")\n",
    "    \n",
    "    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "    output_dir = \"results/mbart_en_hi_bidirectional\"\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_data, val_data = prepare_mbart_data()\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        # Set source language\n",
    "        tokenizer.src_lang = examples['src_lang'][0]  # Assuming batch has same src_lang\n",
    "        \n",
    "        # Tokenize sources\n",
    "        model_inputs = tokenizer(\n",
    "            examples['source'], \n",
    "            max_length=128, \n",
    "            truncation=True, \n",
    "            padding=False\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets with target language\n",
    "        tokenizer.tgt_lang = examples['tgt_lang'][0]\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(\n",
    "                examples['target'], \n",
    "                max_length=128, \n",
    "                truncation=True, \n",
    "                padding=False\n",
    "            )\n",
    "        \n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Group by language pairs for preprocessing\n",
    "    train_en_hi = [x for x in train_data if x['src_lang'] == 'en_XX']\n",
    "    train_hi_en = [x for x in train_data if x['src_lang'] == 'hi_IN']\n",
    "    val_en_hi = [x for x in val_data if x['src_lang'] == 'en_XX']\n",
    "    val_hi_en = [x for x in val_data if x['src_lang'] == 'hi_IN']\n",
    "    \n",
    "    # Create datasets\n",
    "    train_en_hi_ds = Dataset.from_list(train_en_hi).map(preprocess_function, batched=True)\n",
    "    train_hi_en_ds = Dataset.from_list(train_hi_en).map(preprocess_function, batched=True)\n",
    "    val_en_hi_ds = Dataset.from_list(val_en_hi).map(preprocess_function, batched=True)\n",
    "    val_hi_en_ds = Dataset.from_list(val_hi_en).map(preprocess_function, batched=True)\n",
    "    \n",
    "    # Combine datasets\n",
    "    from datasets import concatenate_datasets\n",
    "    train_dataset = concatenate_datasets([train_en_hi_ds, train_hi_en_ds]).shuffle(seed=42)\n",
    "    val_dataset = concatenate_datasets([val_en_hi_ds, val_hi_en_ds]).shuffle(seed=42)\n",
    "    \n",
    "    print(f\"Final dataset sizes: train={len(train_dataset)}, val={len(val_dataset)}\")\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        logging_steps=100,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=500,\n",
    "        predict_with_generate=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting mBART training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Saving mBART model to {output_dir}\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "class MBartTranslator:\n",
    "    \"\"\"mBART-based bidirectional translator.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        print(f\"Loading mBART translator from: {model_path}\")\n",
    "        \n",
    "        self.tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n",
    "        self.model = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Language codes\n",
    "        self.lang_codes = {\n",
    "            'en': 'en_XX',\n",
    "            'hi': 'hi_IN'\n",
    "        }\n",
    "        \n",
    "        print(\"mBART translator ready!\")\n",
    "\n",
    "    def translate(self, text, src_lang='en', tgt_lang='hi'):\n",
    "        \"\"\"Translate using mBART.\"\"\"\n",
    "        src_code = self.lang_codes[src_lang]\n",
    "        tgt_code = self.lang_codes[tgt_lang]\n",
    "        \n",
    "        # Set source language\n",
    "        self.tokenizer.src_lang = src_code\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate with target language\n",
    "        generated_tokens = self.model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=self.tokenizer.lang_code_to_id[tgt_code],\n",
    "            max_length=128,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "        \n",
    "        # Decode\n",
    "        translation = self.tokenizer.batch_decode(\n",
    "            generated_tokens, \n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        return translation.strip()\n",
    "\n",
    "# Fallback: Use pre-trained model without fine-tuning\n",
    "def use_pretrained_mbart():\n",
    "    \"\"\"Use pre-trained mBART without fine-tuning as fallback.\"\"\"\n",
    "    print(\"--- Using Pre-trained mBART (No Fine-tuning) ---\")\n",
    "    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "    \n",
    "    # Create a simple wrapper\n",
    "    class PretrainedMBart:\n",
    "        def __init__(self):\n",
    "            self.tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "            self.model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "        def translate(self, text, src_lang='en', tgt_lang='hi'):\n",
    "            lang_codes = {'en': 'en_XX', 'hi': 'hi_IN'}\n",
    "            src_code = lang_codes[src_lang]\n",
    "            tgt_code = lang_codes[tgt_lang]\n",
    "            \n",
    "            self.tokenizer.src_lang = src_code\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            generated_tokens = self.model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=self.tokenizer.lang_code_to_id[tgt_code],\n",
    "                max_length=128,\n",
    "                num_beams=5,\n",
    "            )\n",
    "            \n",
    "            return self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0].strip()\n",
    "    \n",
    "    return PretrainedMBart()\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self):\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    def calculate_bleu(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(sentence_bleu([ref_tokens], cand_tokens, smoothing_function=self.smoothing) * 100, 2)\n",
    "    \n",
    "    def calculate_meteor(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(meteor_score([ref_tokens], cand_tokens) * 100, 2)\n",
    "\n",
    "def run_evaluation(translator):\n",
    "    \"\"\"Evaluate translation model.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    \n",
    "    # Download NLTK data\n",
    "    for corpus in ['punkt', 'wordnet', 'omw-1.4']:\n",
    "        nltk.download(corpus, quiet=True)\n",
    "\n",
    "    evaluator = TranslationEvaluator()\n",
    "    \n",
    "    test_cases = [\n",
    "        # English to Hindi\n",
    "        {'source': 'Hello', 'reference': 'नमस्ते', 'src_lang': 'en', 'tgt_lang': 'hi'},\n",
    "        {'source': 'How are you?', 'reference': 'आप कैसे हैं?', 'src_lang': 'en', 'tgt_lang': 'hi'},\n",
    "        {'source': 'Good morning', 'reference': 'सुप्रभात', 'src_lang': 'en', 'tgt_lang': 'hi'},\n",
    "        {'source': 'Thank you', 'reference': 'धन्यवाद', 'src_lang': 'en', 'tgt_lang': 'hi'},\n",
    "        {'source': 'I am fine', 'reference': 'मैं ठीक हूं', 'src_lang': 'en', 'tgt_lang': 'hi'},\n",
    "        \n",
    "        # Hindi to English  \n",
    "        {'source': 'नमस्ते', 'reference': 'Hello', 'src_lang': 'hi', 'tgt_lang': 'en'},\n",
    "        {'source': 'आप कैसे हैं?', 'reference': 'How are you?', 'src_lang': 'hi', 'tgt_lang': 'en'},\n",
    "        {'source': 'धन्यवाद', 'reference': 'Thank you', 'src_lang': 'hi', 'tgt_lang': 'en'},\n",
    "        {'source': 'मैं ठीक हूं', 'reference': 'I am fine', 'src_lang': 'hi', 'tgt_lang': 'en'},\n",
    "        {'source': 'सुप्रभात', 'reference': 'Good morning', 'src_lang': 'hi', 'tgt_lang': 'en'},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    print(\"Evaluating translations...\")\n",
    "    \n",
    "    for i, case in enumerate(test_cases, 1):\n",
    "        print(f\"{i:2d}. '{case['source']}' ({case['src_lang']}->{case['tgt_lang']})\")\n",
    "        \n",
    "        try:\n",
    "            prediction = translator.translate(case['source'], case['src_lang'], case['tgt_lang'])\n",
    "            bleu = evaluator.calculate_bleu(case['reference'], prediction)\n",
    "            meteor = evaluator.calculate_meteor(case['reference'], prediction)\n",
    "            \n",
    "            results.append({\n",
    "                'Source': case['source'],\n",
    "                'Reference': case['reference'], \n",
    "                'Prediction': prediction,\n",
    "                'BLEU': bleu,\n",
    "                'METEOR': meteor\n",
    "            })\n",
    "            \n",
    "            print(f\"    → {prediction}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    → ERROR: {str(e)}\")\n",
    "            results.append({\n",
    "                'Source': case['source'],\n",
    "                'Reference': case['reference'],\n",
    "                'Prediction': f\"ERROR: {str(e)}\",\n",
    "                'BLEU': 0.0,\n",
    "                'METEOR': 0.0\n",
    "            })\n",
    "    \n",
    "    # Results summary\n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df.to_string(index=False, max_colwidth=40))\n",
    "    \n",
    "    valid_results = df[df['BLEU'] > 0]\n",
    "    if len(valid_results) > 0:\n",
    "        avg_bleu = valid_results['BLEU'].mean()\n",
    "        avg_meteor = valid_results['METEOR'].mean()\n",
    "        print(f\"\\nAverage BLEU: {avg_bleu:.2f}\")\n",
    "        print(f\"Average METEOR: {avg_meteor:.2f}\")\n",
    "        print(f\"Success rate: {len(valid_results)}/{len(results)} ({100*len(valid_results)/len(results):.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\nNo successful translations generated.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution - try fine-tuning, fallback to pretrained.\"\"\"\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        print(\"Attempting mBART fine-tuning...\")\n",
    "        model_path = fine_tune_mbart()\n",
    "        translator = MBartTranslator(model_path)\n",
    "        print(\"Using fine-tuned mBART model\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fine-tuning failed: {e}\")\n",
    "        print(\"Using pre-trained mBART model...\")\n",
    "        translator = use_pretrained_mbart()\n",
    "    \n",
    "    # Evaluate\n",
    "    run_evaluation(translator)\n",
    "    \n",
    "    return translator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65cba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing mBART translator...\n",
      "Loading mBART translator from: results/mbart_en_hi_bidirectional\n",
      "mBART translator ready!\n",
      "Fine-tuned mBART translator initialized.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.202.209:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:02] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:03] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:05] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:16] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:16] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:23] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:23] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:05:26] \"POST /api/translate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import time\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Global translator instance\n",
    "translator = None\n",
    "LANGUAGES = {'en': 'English', 'hi': 'Hindi'}\n",
    "\n",
    "def initialize_translator():\n",
    "    global translator\n",
    "    print(\"Initializing mBART translator...\")\n",
    "    try:\n",
    "        # Try fine-tuned model first\n",
    "        model_path = \"results/mbart_en_hi_bidirectional\"\n",
    "        translator = MBartTranslator(model_path)\n",
    "        print(\"Fine-tuned mBART translator initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fine-tuned model not found: {e}\")\n",
    "        print(\"Using pre-trained mBART model...\")\n",
    "        try:\n",
    "            # Fallback to pre-trained mBART\n",
    "            class PretrainedMBart:\n",
    "                def __init__(self):\n",
    "                    from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "                    model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "                    self.tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "                    self.model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "                    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    self.model.to(self.device)\n",
    "                    self.model.eval()\n",
    "                    \n",
    "                def translate(self, text, src_lang='en', tgt_lang='hi'):\n",
    "                    lang_codes = {'en': 'en_XX', 'hi': 'hi_IN'}\n",
    "                    src_code = lang_codes[src_lang]\n",
    "                    tgt_code = lang_codes[tgt_lang]\n",
    "                    \n",
    "                    self.tokenizer.src_lang = src_code\n",
    "                    inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    generated_tokens = self.model.generate(\n",
    "                        **inputs,\n",
    "                        forced_bos_token_id=self.tokenizer.lang_code_to_id[tgt_code],\n",
    "                        max_length=128,\n",
    "                        num_beams=5,\n",
    "                    )\n",
    "                    \n",
    "                    return self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0].strip()\n",
    "            \n",
    "            translator = PretrainedMBart()\n",
    "            print(\"Pre-trained mBART translator initialized.\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to initialize any translator: {e2}\")\n",
    "            translator = None\n",
    "\n",
    "@app.route('/api/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'translator_ready': translator is not None,\n",
    "        'supported_languages': LANGUAGES\n",
    "    })\n",
    "\n",
    "@app.route('/api/translate', methods=['POST'])\n",
    "def translate_text():\n",
    "    try:\n",
    "        if translator is None:\n",
    "            return jsonify({'error': 'Translator not initialized. Please check model path.'}), 503\n",
    "        \n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        text = data.get('text', '').strip()\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "\n",
    "        if not text:\n",
    "            return jsonify({'error': 'No text provided for translation.'}), 400\n",
    "        if src_lang not in LANGUAGES or tgt_lang not in LANGUAGES:\n",
    "            return jsonify({'error': 'Unsupported language selected.'}), 400\n",
    "        if src_lang == tgt_lang:\n",
    "            return jsonify({'error': 'Source and target languages are the same.'}), 400\n",
    "\n",
    "        start_time = time.time()\n",
    "        translation = translator.translate(text, src_lang, tgt_lang)\n",
    "        end_time = time.time()\n",
    "\n",
    "        return jsonify({\n",
    "            'source_text': text,\n",
    "            'source_language': src_lang,\n",
    "            'target_language': tgt_lang,\n",
    "            'translation': translation,\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/batch-translate', methods=['POST'])\n",
    "def batch_translate():\n",
    "    try:\n",
    "        if translator is None:\n",
    "            return jsonify({'error': 'Translator not initialized.'}), 503\n",
    "            \n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        texts = data.get('texts', [])\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "        \n",
    "        if not texts or not isinstance(texts, list):\n",
    "            return jsonify({'error': 'No texts array provided.'}), 400\n",
    "        if len(texts) > 100:\n",
    "            return jsonify({'error': 'Maximum 100 texts allowed per batch.'}), 400\n",
    "\n",
    "        start_time = time.time()\n",
    "        translations = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if isinstance(text, str) and text.strip():\n",
    "                try:\n",
    "                    translation = translator.translate(text.strip(), src_lang, tgt_lang)\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': translation,\n",
    "                        'success': True\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': None,\n",
    "                        'success': False,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "            else:\n",
    "                translations.append({\n",
    "                    'source': text,\n",
    "                    'translation': None,\n",
    "                    'success': False,\n",
    "                    'error': 'Invalid text format'\n",
    "                })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        return jsonify({\n",
    "            'translations': translations,\n",
    "            'total_count': len(translations),\n",
    "            'success_count': sum(1 for t in translations if t['success']),\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Batch translation error: {e}\")\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/languages', methods=['GET'])\n",
    "def get_languages():\n",
    "    return jsonify({\n",
    "        'supported_languages': LANGUAGES,\n",
    "        'translation_pairs': [\n",
    "            {'source': 'en', 'target': 'hi'},\n",
    "            {'source': 'hi', 'target': 'en'}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "def run_flask_app():\n",
    "    initialize_translator()\n",
    "    app.run(debug=False, use_reloader=False, host='0.0.0.0', port=5000, threaded=True)\n",
    "\n",
    "def start_api_server():\n",
    "    thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "    thread.start()\n",
    "    return thread\n",
    "\n",
    "# For Jupyter notebook usage\n",
    "def start_translation_api():\n",
    "    print(\"Starting translation API server...\")\n",
    "    thread = start_api_server()\n",
    "    print(\"API server started on http://localhost:5000\")\n",
    "    print(\"\\nAvailable endpoints:\")\n",
    "    print(\"  GET  /api/health - Check server status\")\n",
    "    print(\"  GET  /api/languages - Get supported languages\")\n",
    "    print(\"  POST /api/translate - Translate single text\")\n",
    "    print(\"  POST /api/batch-translate - Translate multiple texts\")\n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"curl -X POST http://localhost:5000/api/translate \\\\\")\n",
    "    print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "    print(\"  -d '{\\\"text\\\": \\\"Hello\\\", \\\"src_lang\\\": \\\"en\\\", \\\"tgt_lang\\\": \\\"hi\\\"}'\")\n",
    "    return thread\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_flask_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt_rerun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
