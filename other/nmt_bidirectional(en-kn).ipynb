{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534c6f8a",
   "metadata": {},
   "source": [
    "## Fine-tune MarianMT model for English-Kannada translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096f91d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training English-Kannada model...\n",
      "--- Starting Fine-Tuning MarianMT English-Kannada ---\n",
      "Loading Helsinki-NLP/opus-mt-en-mul...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88bbf4e46a04d2791418d00b424fd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4d4710b2294cdcbcc36264bd5e6d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6605cf3d6894f37b18e0000bd20d0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6006918f105c4e2f96e2b41c4b4a3e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39d04d4f9e1481fa1000460ab5ecb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b774d4b8ca4cc3809644ebb2ad46b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fd62f24dfa4fddaae5caa9510c5654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading samanantar dataset...\n",
      "Found 5000 valid translation pairs\n",
      "Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b993d8b62044b6a713403750d4058d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc912674dd164fc293baeffc7b678d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting English-Kannada training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 00:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.461700</td>\n",
       "      <td>2.213240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.158300</td>\n",
       "      <td>2.078920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.956900</td>\n",
       "      <td>2.021572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.960800</td>\n",
       "      <td>1.994814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving English-Kannada model to results/marian_en_kn_finetuned\n",
      "Training Kannada-English model...\n",
      "--- Starting Fine-Tuning MarianMT Kannada-English ---\n",
      "Loading Helsinki-NLP/opus-mt-mul-en...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be48595bfe846d6887f6479b9cd3861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ad32efdd4f45a2b14c717bdf1bd9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/707k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b2dcafa6a043319005e9f99fae91ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/791k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df37200376e842fd8183b7cde2b6d3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bb81710fe94fbc93f1f3652d9ffd75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d18a902718f49808bd90f6ba6309ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/310M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d418836212bb4103ba526e41209904c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading samanantar dataset...\n",
      "Found 5000 valid translation pairs\n",
      "Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a49de998c224e1d92aa3420667c4248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad920328d3748e5a2283476a7ed5a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Kannada-English training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 00:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.074400</td>\n",
       "      <td>2.816344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.533400</td>\n",
       "      <td>2.703010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.164600</td>\n",
       "      <td>2.672754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.109500</td>\n",
       "      <td>2.651908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Kannada-English model to results/marian_kn_en_finetuned\n",
      "\n",
      "--- Starting Evaluation ---\n",
      "\n",
      "--- Evaluating English-Kannada Translation ---\n",
      "Loading translator from: results/marian_en_kn_finetuned\n",
      "Detected model type: marian\n",
      "Translator ready!\n",
      " 1. 'Hello'\n",
      "    → ಹಾಲೊ\n",
      " 2. 'How are you?'\n",
      "    → ನೀವು ಹೇಗೆ?\n",
      " 3. 'Good morning'\n",
      "    → ಮೊದಲು ಮೊದಲು\n",
      " 4. 'Thank you'\n",
      "    → ಧನ್ಯತೆ\n",
      " 5. 'I am fine'\n",
      "    → ನನಗೆ ಒಳ್ಳೆಯದು.\n",
      "\n",
      "================================================================================\n",
      "ENGLISH-KANNADA EVALUATION RESULTS\n",
      "================================================================================\n",
      "      Source           Reference     Prediction  BLEU  METEOR\n",
      "       Hello             ನಮಸ್ಕಾರ           ಹಾಲೊ  0.00    0.00\n",
      "How are you?         ಹೇಗಿದ್ದೀರಾ?     ನೀವು ಹೇಗೆ? 11.36   23.81\n",
      "Good morning              ಶುಭೋದಯ    ಮೊದಲು ಮೊದಲು  0.00    0.00\n",
      "   Thank you          ಧನ್ಯವಾದಗಳು         ಧನ್ಯತೆ  0.00    0.00\n",
      "   I am fine ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ ನನಗೆ ಒಳ್ಳೆಯದು.  0.00    0.00\n",
      "\n",
      "Average BLEU: 11.36\n",
      "Average METEOR: 23.81\n",
      "Success rate: 1/5 (20.0%)\n",
      "\n",
      "--- Evaluating Kannada-English Translation ---\n",
      "Loading translator from: results/marian_kn_en_finetuned\n",
      "Detected model type: marian\n",
      "Translator ready!\n",
      " 1. 'ನಮಸ್ಕಾರ'\n",
      "    → Namskar\n",
      " 2. 'ಹೇಗಿದ್ದೀರಾ?'\n",
      "    → How are you?\n",
      " 3. 'ಶುಭೋದಯ'\n",
      "    → Good luck\n",
      " 4. 'ಧನ್ಯವಾದಗಳು'\n",
      "    → Thank you.\n",
      " 5. 'ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ'\n",
      "    → I am fine\n",
      "\n",
      "================================================================================\n",
      "KANNADA-ENGLISH EVALUATION RESULTS\n",
      "================================================================================\n",
      "             Source    Reference   Prediction   BLEU  METEOR\n",
      "            ನಮಸ್ಕಾರ        Hello      Namskar   0.00    0.00\n",
      "        ಹೇಗಿದ್ದೀರಾ? How are you? How are you? 100.00   99.22\n",
      "             ಶುಭೋದಯ Good morning    Good luck  14.95   25.00\n",
      "         ಧನ್ಯವಾದಗಳು    Thank you   Thank you.  24.03   89.29\n",
      "ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ    I am fine    I am fine  56.23   98.15\n",
      "\n",
      "Average BLEU: 48.80\n",
      "Average METEOR: 77.91\n",
      "Success rate: 4/5 (80.0%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def prepare_translation_data():\n",
    "    \"\"\"Prepare translation data from samanantar dataset.\"\"\"\n",
    "    print(\"Loading samanantar dataset...\")\n",
    "    dataset = load_dataset(\"ai4bharat/samanantar\", \"kn\", split='train')\n",
    "    \n",
    "    # Filter valid pairs and create smaller dataset for testing\n",
    "    valid_pairs = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= 5000:  # Use smaller dataset for debugging\n",
    "            break\n",
    "        if example['src'] and example['tgt'] and len(example['src'].strip()) > 0 and len(example['tgt'].strip()) > 0:\n",
    "            valid_pairs.append({\n",
    "                'english': example['src'].strip(),\n",
    "                'kannada': example['tgt'].strip()\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(valid_pairs)} valid translation pairs\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_idx = int(0.9 * len(valid_pairs))\n",
    "    train_pairs = valid_pairs[:split_idx]\n",
    "    val_pairs = valid_pairs[split_idx:]\n",
    "    \n",
    "    return train_pairs, val_pairs\n",
    "\n",
    "def fine_tune_marian_en_kn():\n",
    "    \"\"\"Fine-tune MarianMT model for English-Kannada translation.\"\"\"\n",
    "    print(\"--- Starting Fine-Tuning MarianMT English-Kannada ---\")\n",
    "    \n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "    output_dir = \"results/marian_en_kn_finetuned\"\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Prepare data\n",
    "        train_pairs, val_pairs = prepare_translation_data()\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            inputs = examples[\"english\"]\n",
    "            targets = examples[\"kannada\"] \n",
    "            model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "            \n",
    "            # Setup the tokenizer for targets\n",
    "            labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        # Convert to datasets\n",
    "        train_dataset = Dataset.from_list(train_pairs)\n",
    "        val_dataset = Dataset.from_list(val_pairs)\n",
    "        \n",
    "        print(\"Preprocessing datasets...\")\n",
    "        train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            logging_steps=50,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=200,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting English-Kannada training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        print(f\"Saving English-Kannada model to {output_dir}\")\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MarianMT English-Kannada failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def fine_tune_marian_kn_en():\n",
    "    \"\"\"Fine-tune MarianMT model for Kannada-English translation.\"\"\"\n",
    "    print(\"--- Starting Fine-Tuning MarianMT Kannada-English ---\")\n",
    "    \n",
    "    model_name = \"Helsinki-NLP/opus-mt-mul-en\"\n",
    "    output_dir = \"results/marian_kn_en_finetuned\"\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Prepare data (reversed for Kannada to English)\n",
    "        train_pairs, val_pairs = prepare_translation_data()\n",
    "        \n",
    "        # Reverse the pairs for Kannada-English translation\n",
    "        train_pairs_reversed = [{'english': pair['kannada'], 'kannada': pair['english']} for pair in train_pairs]\n",
    "        val_pairs_reversed = [{'english': pair['kannada'], 'kannada': pair['english']} for pair in val_pairs]\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            inputs = examples[\"english\"]  # Now contains Kannada text\n",
    "            targets = examples[\"kannada\"]   # Now contains English text\n",
    "            model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "            \n",
    "            # Setup the tokenizer for targets\n",
    "            labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        # Convert to datasets\n",
    "        train_dataset = Dataset.from_list(train_pairs_reversed)\n",
    "        val_dataset = Dataset.from_list(val_pairs_reversed)\n",
    "        \n",
    "        print(\"Preprocessing datasets...\")\n",
    "        train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            logging_steps=50,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=200,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting Kannada-English training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        print(f\"Saving Kannada-English model to {output_dir}\")\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MarianMT Kannada-English failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def fine_tune_t5_model():\n",
    "    \"\"\"Fallback: Fine-tune T5 model for bidirectional translation.\"\"\"\n",
    "    from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "    \n",
    "    model_name = \"t5-small\"\n",
    "    output_dir = \"results/t5_bidirectional_finetuned\"\n",
    "    \n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_pairs, val_pairs = prepare_translation_data()\n",
    "    \n",
    "    # Create bidirectional training data with T5 format\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    # English to Kannada\n",
    "    for pair in train_pairs:\n",
    "        train_data.append({\n",
    "            'input_text': f\"translate English to Kannada: {pair['english']}\",\n",
    "            'target_text': pair['kannada']\n",
    "        })\n",
    "        # Kannada to English (reverse)\n",
    "        train_data.append({\n",
    "            'input_text': f\"translate Kannada to English: {pair['kannada']}\",\n",
    "            'target_text': pair['english']\n",
    "        })\n",
    "    \n",
    "    for pair in val_pairs:\n",
    "        val_data.append({\n",
    "            'input_text': f\"translate English to Kannada: {pair['english']}\",\n",
    "            'target_text': pair['kannada']\n",
    "        })\n",
    "        val_data.append({\n",
    "            'input_text': f\"translate Kannada to English: {pair['kannada']}\",\n",
    "            'target_text': pair['english']\n",
    "        })\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"input_text\"]\n",
    "        targets = examples[\"target_text\"]\n",
    "        model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "        labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Convert to datasets\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    \n",
    "    print(\"Preprocessing datasets...\")\n",
    "    train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "    val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        logging_steps=50,\n",
    "        save_steps=200,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=200,\n",
    "        predict_with_generate=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting T5 bidirectional training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Saving T5 model to {output_dir}\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "class UniversalTranslator:\n",
    "    \"\"\"Universal translator that works with different model types.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        print(f\"Loading translator from: {model_path}\")\n",
    "        \n",
    "        # Detect model type\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        self.model_type = \"unknown\"\n",
    "        \n",
    "        if os.path.exists(config_path):\n",
    "            import json\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                if \"marian\" in config.get(\"architectures\", [\"\"])[0].lower():\n",
    "                    self.model_type = \"marian\"\n",
    "                elif \"t5\" in config.get(\"architectures\", [\"\"])[0].lower():\n",
    "                    self.model_type = \"t5\"\n",
    "        \n",
    "        print(f\"Detected model type: {self.model_type}\")\n",
    "        \n",
    "        if self.model_type == \"marian\":\n",
    "            self.tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "            self.model = MarianMTModel.from_pretrained(model_path)\n",
    "        else:  # Default to T5\n",
    "            from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "            self.model_type = \"t5\"\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"Translator ready!\")\n",
    "\n",
    "    def translate(self, text, src_lang='en', tgt_lang='kn'):\n",
    "        \"\"\"Translate text based on model type.\"\"\"\n",
    "        if self.model_type == \"marian\":\n",
    "            # MarianMT expects plain text\n",
    "            input_text = text\n",
    "        else:  # T5\n",
    "            lang_map = {'en': 'English', 'kn': 'Kannada'}\n",
    "            input_text = f\"translate {lang_map[src_lang]} to {lang_map[tgt_lang]}: {text}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                length_penalty=0.6,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation.strip()\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self):\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    def calculate_bleu(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(sentence_bleu([ref_tokens], cand_tokens, smoothing_function=self.smoothing) * 100, 2)\n",
    "    \n",
    "    def calculate_meteor(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(meteor_score([ref_tokens], cand_tokens) * 100, 2)\n",
    "\n",
    "def run_evaluation(model_paths):\n",
    "    \"\"\"Evaluate the trained models.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    \n",
    "    # Download NLTK data\n",
    "    for corpus in ['punkt', 'wordnet', 'omw-1.4']:\n",
    "        nltk.download(corpus, quiet=True)\n",
    "\n",
    "    evaluator = TranslationEvaluator()\n",
    "    \n",
    "    # Test cases for both directions\n",
    "    en_to_kn_cases = [\n",
    "        {'source': 'Hello', 'reference': 'ನಮಸ್ಕಾರ'},\n",
    "        {'source': 'How are you?', 'reference': 'ಹೇಗಿದ್ದೀರಾ?'},\n",
    "        {'source': 'Good morning', 'reference': 'ಶುಭೋದಯ'},\n",
    "        {'source': 'Thank you', 'reference': 'ಧನ್ಯವಾದಗಳು'},\n",
    "        {'source': 'I am fine', 'reference': 'ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ'},\n",
    "    ]\n",
    "    \n",
    "    kn_to_en_cases = [\n",
    "        {'source': 'ನಮಸ್ಕಾರ', 'reference': 'Hello'},\n",
    "        {'source': 'ಹೇಗಿದ್ದೀರಾ?', 'reference': 'How are you?'},\n",
    "        {'source': 'ಶುಭೋದಯ', 'reference': 'Good morning'},\n",
    "        {'source': 'ಧನ್ಯವಾದಗಳು', 'reference': 'Thank you'},\n",
    "        {'source': 'ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ', 'reference': 'I am fine'},\n",
    "    ]\n",
    "    \n",
    "    # Evaluate both directions\n",
    "    for direction, cases, model_path in [\n",
    "        (\"English-Kannada\", en_to_kn_cases, model_paths.get('en_kn')),\n",
    "        (\"Kannada-English\", kn_to_en_cases, model_paths.get('kn_en'))\n",
    "    ]:\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(f\"Model not found for {direction}: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- Evaluating {direction} Translation ---\")\n",
    "        translator = UniversalTranslator(model_path)\n",
    "        results = []\n",
    "        \n",
    "        for i, case in enumerate(cases, 1):\n",
    "            print(f\"{i:2d}. '{case['source']}'\")\n",
    "            \n",
    "            try:\n",
    "                src_lang = 'en' if direction == \"English-Kannada\" else 'kn'\n",
    "                tgt_lang = 'kn' if direction == \"English-Kannada\" else 'en'\n",
    "                \n",
    "                prediction = translator.translate(case['source'], src_lang, tgt_lang)\n",
    "                bleu = evaluator.calculate_bleu(case['reference'], prediction)\n",
    "                meteor = evaluator.calculate_meteor(case['reference'], prediction)\n",
    "                \n",
    "                results.append({\n",
    "                    'Source': case['source'],\n",
    "                    'Reference': case['reference'], \n",
    "                    'Prediction': prediction,\n",
    "                    'BLEU': bleu,\n",
    "                    'METEOR': meteor\n",
    "                })\n",
    "                \n",
    "                print(f\"    → {prediction}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    → ERROR: {str(e)}\")\n",
    "                results.append({\n",
    "                    'Source': case['source'],\n",
    "                    'Reference': case['reference'],\n",
    "                    'Prediction': f\"ERROR: {str(e)}\",\n",
    "                    'BLEU': 0.0,\n",
    "                    'METEOR': 0.0\n",
    "                })\n",
    "        \n",
    "        # Results summary\n",
    "        df = pd.DataFrame(results)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{direction.upper()} EVALUATION RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(df.to_string(index=False, max_colwidth=40))\n",
    "        \n",
    "        valid_results = df[df['BLEU'] > 0]\n",
    "        if len(valid_results) > 0:\n",
    "            avg_bleu = valid_results['BLEU'].mean()\n",
    "            avg_meteor = valid_results['METEOR'].mean()\n",
    "            print(f\"\\nAverage BLEU: {avg_bleu:.2f}\")\n",
    "            print(f\"Average METEOR: {avg_meteor:.2f}\")\n",
    "            print(f\"Success rate: {len(valid_results)}/{len(results)} ({100*len(valid_results)/len(results):.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\nNo successful translations generated.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    model_paths = {}\n",
    "    \n",
    "    # Train English-Kannada model\n",
    "    print(\"Training English-Kannada model...\")\n",
    "    en_kn_path = fine_tune_marian_en_kn()\n",
    "    if en_kn_path:\n",
    "        model_paths['en_kn'] = en_kn_path\n",
    "    \n",
    "    # Train Kannada-English model\n",
    "    print(\"Training Kannada-English model...\")\n",
    "    kn_en_path = fine_tune_marian_kn_en()\n",
    "    if kn_en_path:\n",
    "        model_paths['kn_en'] = kn_en_path\n",
    "    \n",
    "    # If both MarianMT models fail, train T5 as fallback\n",
    "    if not model_paths:\n",
    "        print(\"Both MarianMT models failed. Training T5 fallback...\")\n",
    "        t5_path = fine_tune_t5_model()\n",
    "        if t5_path:\n",
    "            model_paths['en_kn'] = t5_path\n",
    "            model_paths['kn_en'] = t5_path\n",
    "    \n",
    "    # Evaluate models\n",
    "    if model_paths:\n",
    "        run_evaluation(model_paths)\n",
    "    else:\n",
    "        print(\"No models were successfully trained.\")\n",
    "    \n",
    "    return model_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea1d18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing translators...\n",
      "Loading marian model from: results/marian_en_hi_finetuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/nmt_rerun/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully: marian\n",
      "Cached translator for en -> hi\n",
      "✓ Initialized en -> hi translator\n",
      "Loading marian model from: results/marian_hi_en_finetuned\n",
      "Model loaded successfully: marian\n",
      "Cached translator for hi -> en\n",
      "✓ Initialized hi -> en translator\n",
      "Translator initialization complete.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.202.209:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [19/Aug/2025 11:13:17] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading marian model from: results/marian_en_kn_finetuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [19/Aug/2025 11:13:18] \"POST /api/translate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully: marian\n",
      "Cached translator for en -> kn\n",
      "Loading marian model from: results/marian_kn_en_finetuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [19/Aug/2025 11:13:22] \"POST /api/translate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully: marian\n",
      "Cached translator for kn -> en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [19/Aug/2025 11:13:25] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 11:13:25] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 11:13:33] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 11:13:33] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 11:13:49] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 11:13:49] \"POST /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 11:13:57] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 11:13:58] \"POST /api/translate HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from transformers import MarianMTModel, MarianTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Global translator cache and configuration\n",
    "translator_cache = {}\n",
    "LANGUAGES = {'en': 'English', 'hi': 'Hindi', 'kn': 'Kannada'}\n",
    "MODEL_PATHS = {\n",
    "    'en_hi': 'results/marian_en_hi_finetuned',\n",
    "    'hi_en': 'results/marian_hi_en_finetuned',\n",
    "    'en_kn': 'results/marian_en_kn_finetuned',\n",
    "    'kn_en': 'results/marian_kn_en_finetuned',\n",
    "    'fallback': 'results/t5_bidirectional_finetuned'\n",
    "}\n",
    "\n",
    "class DynamicTranslator:\n",
    "    \"\"\"Dynamic translator that loads models on demand based on language pair.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.model_type = self._detect_model_type()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._load_model()\n",
    "    \n",
    "    def _detect_model_type(self):\n",
    "        \"\"\"Detect model type from config.json\"\"\"\n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        \n",
    "        if os.path.exists(config_path):\n",
    "            try:\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config = json.load(f)\n",
    "                    architecture = config.get(\"architectures\", [\"\"])[0].lower()\n",
    "                    if \"marian\" in architecture:\n",
    "                        return \"marian\"\n",
    "                    elif \"t5\" in architecture:\n",
    "                        return \"t5\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading config: {e}\")\n",
    "        \n",
    "        # Fallback detection based on path\n",
    "        if \"marian\" in self.model_path.lower():\n",
    "            return \"marian\"\n",
    "        elif \"t5\" in self.model_path.lower():\n",
    "            return \"t5\"\n",
    "        \n",
    "        return \"unknown\"\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model and tokenizer based on detected type.\"\"\"\n",
    "        print(f\"Loading {self.model_type} model from: {self.model_path}\")\n",
    "        \n",
    "        try:\n",
    "            if self.model_type == \"marian\":\n",
    "                self.tokenizer = MarianTokenizer.from_pretrained(self.model_path)\n",
    "                self.model = MarianMTModel.from_pretrained(self.model_path)\n",
    "            else:  # T5 or unknown (default to T5)\n",
    "                self.tokenizer = T5Tokenizer.from_pretrained(self.model_path)\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(self.model_path)\n",
    "                self.model_type = \"t5\"\n",
    "            \n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(f\"Model loaded successfully: {self.model_type}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def translate(self, text, src_lang='en', tgt_lang='hi'):\n",
    "        \"\"\"Translate text based on model type.\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise Exception(\"Model not loaded\")\n",
    "        \n",
    "        # Prepare input based on model type\n",
    "        if self.model_type == \"marian\":\n",
    "            input_text = text\n",
    "        else:  # T5\n",
    "            lang_map = {'en': 'English', 'hi': 'Hindi'}\n",
    "            input_text = f\"translate {lang_map[src_lang]} to {lang_map[tgt_lang]}: {text}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                length_penalty=0.6,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        # Decode and return\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation.strip()\n",
    "\n",
    "def get_model_path(src_lang, tgt_lang):\n",
    "    \"\"\"Get appropriate model path based on language pair.\"\"\"\n",
    "    language_pair = f\"{src_lang}_{tgt_lang}\"\n",
    "    \n",
    "    # Check if specific directional model exists\n",
    "    if language_pair in MODEL_PATHS and os.path.exists(MODEL_PATHS[language_pair]):\n",
    "        return MODEL_PATHS[language_pair]\n",
    "    \n",
    "    # Check if fallback model exists\n",
    "    if os.path.exists(MODEL_PATHS['fallback']):\n",
    "        return MODEL_PATHS['fallback']\n",
    "    \n",
    "    # Try to find any available model\n",
    "    for path in MODEL_PATHS.values():\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_translator(src_lang, tgt_lang):\n",
    "    \"\"\"Get or create translator for specific language pair.\"\"\"\n",
    "    cache_key = f\"{src_lang}_{tgt_lang}\"\n",
    "    \n",
    "    # Return cached translator if available\n",
    "    if cache_key in translator_cache:\n",
    "        return translator_cache[cache_key]\n",
    "    \n",
    "    # Get model path\n",
    "    model_path = get_model_path(src_lang, tgt_lang)\n",
    "    \n",
    "    if not model_path:\n",
    "        raise Exception(f\"No model available for {src_lang} -> {tgt_lang} translation\")\n",
    "    \n",
    "    # Create and cache new translator\n",
    "    try:\n",
    "        translator = DynamicTranslator(model_path)\n",
    "        translator_cache[cache_key] = translator\n",
    "        print(f\"Cached translator for {src_lang} -> {tgt_lang}\")\n",
    "        return translator\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load translator: {str(e)}\")\n",
    "\n",
    "def initialize_translators():\n",
    "    \"\"\"Pre-initialize common translators.\"\"\"\n",
    "    print(\"Initializing translators...\")\n",
    "    \n",
    "    common_pairs = [('en', 'hi'), ('hi', 'en')]\n",
    "    \n",
    "    for src_lang, tgt_lang in common_pairs:\n",
    "        try:\n",
    "            get_translator(src_lang, tgt_lang)\n",
    "            print(f\"✓ Initialized {src_lang} -> {tgt_lang} translator\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to initialize {src_lang} -> {tgt_lang}: {e}\")\n",
    "    \n",
    "    print(\"Translator initialization complete.\")\n",
    "\n",
    "@app.route('/api/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint with detailed model status.\"\"\"\n",
    "    model_status = {}\n",
    "    \n",
    "    for pair, path in MODEL_PATHS.items():\n",
    "        model_status[pair] = {\n",
    "            'path': path,\n",
    "            'exists': os.path.exists(path),\n",
    "            'cached': f\"{pair.replace('_', '_')}\" in translator_cache if '_' in pair else False\n",
    "        }\n",
    "    \n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'supported_languages': LANGUAGES,\n",
    "        'model_status': model_status,\n",
    "        'cached_translators': list(translator_cache.keys()),\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    })\n",
    "\n",
    "@app.route('/api/translate', methods=['POST'])\n",
    "def translate_text():\n",
    "    \"\"\"Single text translation endpoint.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        text = data.get('text', '').strip()\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "\n",
    "        # Validation\n",
    "        if not text:\n",
    "            return jsonify({'error': 'No text provided for translation.'}), 400\n",
    "        if src_lang not in LANGUAGES or tgt_lang not in LANGUAGES:\n",
    "            return jsonify({'error': 'Unsupported language selected.'}), 400\n",
    "        if src_lang == tgt_lang:\n",
    "            return jsonify({'error': 'Source and target languages are the same.'}), 400\n",
    "\n",
    "        # Get translator and translate\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            translator = get_translator(src_lang, tgt_lang)\n",
    "            translation = translator.translate(text, src_lang, tgt_lang)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': f'Translation failed: {str(e)}'}), 503\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        return jsonify({\n",
    "            'source_text': text,\n",
    "            'source_language': src_lang,\n",
    "            'source_language_name': LANGUAGES[src_lang],\n",
    "            'target_language': tgt_lang,\n",
    "            'target_language_name': LANGUAGES[tgt_lang],\n",
    "            'translation': translation,\n",
    "            'model_used': get_model_path(src_lang, tgt_lang),\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/batch-translate', methods=['POST'])\n",
    "def batch_translate():\n",
    "    \"\"\"Batch translation endpoint.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        texts = data.get('texts', [])\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "        \n",
    "        # Validation\n",
    "        if not texts or not isinstance(texts, list):\n",
    "            return jsonify({'error': 'No texts array provided.'}), 400\n",
    "        if len(texts) > 100:\n",
    "            return jsonify({'error': 'Maximum 100 texts allowed per batch.'}), 400\n",
    "        if src_lang not in LANGUAGES or tgt_lang not in LANGUAGES:\n",
    "            return jsonify({'error': 'Unsupported language selected.'}), 400\n",
    "        if src_lang == tgt_lang:\n",
    "            return jsonify({'error': 'Source and target languages are the same.'}), 400\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get translator\n",
    "        try:\n",
    "            translator = get_translator(src_lang, tgt_lang)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': f'Failed to load translator: {str(e)}'}), 503\n",
    "        \n",
    "        translations = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if isinstance(text, str) and text.strip():\n",
    "                try:\n",
    "                    translation = translator.translate(text.strip(), src_lang, tgt_lang)\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': translation,\n",
    "                        'success': True\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': None,\n",
    "                        'success': False,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "            else:\n",
    "                translations.append({\n",
    "                    'source': text,\n",
    "                    'translation': None,\n",
    "                    'success': False,\n",
    "                    'error': 'Invalid text format'\n",
    "                })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        return jsonify({\n",
    "            'translations': translations,\n",
    "            'source_language': src_lang,\n",
    "            'target_language': tgt_lang,\n",
    "            'model_used': get_model_path(src_lang, tgt_lang),\n",
    "            'total_count': len(translations),\n",
    "            'success_count': sum(1 for t in translations if t['success']),\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Batch translation error: {e}\")\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/languages', methods=['GET'])\n",
    "def get_languages():\n",
    "    \"\"\"Get supported languages and available translation pairs.\"\"\"\n",
    "    available_pairs = []\n",
    "    \n",
    "    # Check which models are actually available\n",
    "    for src in LANGUAGES.keys():\n",
    "        for tgt in LANGUAGES.keys():\n",
    "            if src != tgt:\n",
    "                model_path = get_model_path(src, tgt)\n",
    "                if model_path:\n",
    "                    available_pairs.append({\n",
    "                        'source': src,\n",
    "                        'target': tgt,\n",
    "                        'source_name': LANGUAGES[src],\n",
    "                        'target_name': LANGUAGES[tgt],\n",
    "                        'model_path': model_path\n",
    "                    })\n",
    "    \n",
    "    return jsonify({\n",
    "        'supported_languages': LANGUAGES,\n",
    "        'available_translation_pairs': available_pairs,\n",
    "        'total_pairs': len(available_pairs)\n",
    "    })\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "def get_models():\n",
    "    \"\"\"Get information about available models.\"\"\"\n",
    "    models_info = {}\n",
    "    \n",
    "    for model_key, path in MODEL_PATHS.items():\n",
    "        model_info = {\n",
    "            'path': path,\n",
    "            'exists': os.path.exists(path),\n",
    "            'type': 'unknown',\n",
    "            'cached': False\n",
    "        }\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            # Detect model type\n",
    "            config_path = os.path.join(path, \"config.json\")\n",
    "            if os.path.exists(config_path):\n",
    "                try:\n",
    "                    with open(config_path, 'r') as f:\n",
    "                        config = json.load(f)\n",
    "                        architecture = config.get(\"architectures\", [\"\"])[0].lower()\n",
    "                        if \"marian\" in architecture:\n",
    "                            model_info['type'] = \"marian\"\n",
    "                        elif \"t5\" in architecture:\n",
    "                            model_info['type'] = \"t5\"\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Check if cached\n",
    "            cache_key = model_key.replace('_', '_')\n",
    "            model_info['cached'] = cache_key in translator_cache\n",
    "        \n",
    "        models_info[model_key] = model_info\n",
    "    \n",
    "    return jsonify({\n",
    "        'models': models_info,\n",
    "        'cache_status': list(translator_cache.keys())\n",
    "    })\n",
    "\n",
    "@app.route('/api/clear-cache', methods=['POST'])\n",
    "def clear_cache():\n",
    "    \"\"\"Clear translator cache.\"\"\"\n",
    "    global translator_cache\n",
    "    \n",
    "    # Get current cache size\n",
    "    cache_size = len(translator_cache)\n",
    "    \n",
    "    # Clear cache\n",
    "    translator_cache.clear()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return jsonify({\n",
    "        'message': 'Cache cleared successfully',\n",
    "        'cleared_translators': cache_size,\n",
    "        'current_cache_size': len(translator_cache)\n",
    "    })\n",
    "\n",
    "@app.route('/api/warmup', methods=['POST'])\n",
    "def warmup_translators():\n",
    "    \"\"\"Warm up translators for better performance.\"\"\"\n",
    "    data = request.json or {}\n",
    "    language_pairs = data.get('pairs', [('en', 'hi'), ('hi', 'en'), ('en', 'kn'), ('kn', 'en')])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for src_lang, tgt_lang in language_pairs:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            translator = get_translator(src_lang, tgt_lang)\n",
    "            \n",
    "            # Perform a dummy translation to warm up\n",
    "            test_text = \"Hello\" if src_lang == 'en' else \"नमस्ते\"\n",
    "            translator.translate(test_text, src_lang, tgt_lang)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            results.append({\n",
    "                'pair': f\"{src_lang} -> {tgt_lang}\",\n",
    "                'success': True,\n",
    "                'warmup_time': round(end_time - start_time, 3)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'pair': f\"{src_lang} -> {tgt_lang}\",\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return jsonify({\n",
    "        'message': 'Warmup completed',\n",
    "        'results': results,\n",
    "        'total_cached': len(translator_cache)\n",
    "    })\n",
    "\n",
    "def run_flask_app():\n",
    "    \"\"\"Run Flask application.\"\"\"\n",
    "    initialize_translators()\n",
    "    app.run(debug=False, use_reloader=False, host='0.0.0.0', port=5000, threaded=True)\n",
    "\n",
    "def start_api_server():\n",
    "    \"\"\"Start API server in a thread.\"\"\"\n",
    "    thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "    thread.start()\n",
    "    return thread\n",
    "\n",
    "def start_translation_api():\n",
    "    \"\"\"Start translation API for Jupyter notebook usage.\"\"\"\n",
    "    print(\"Starting enhanced translation API server...\")\n",
    "    thread = start_api_server()\n",
    "    print(\"API server started on http://localhost:5000\")\n",
    "    print(\"\\nAvailable endpoints:\")\n",
    "    print(\"  GET  /api/health - Check server status and model availability\")\n",
    "    print(\"  GET  /api/languages - Get supported languages and available pairs\")\n",
    "    print(\"  GET  /api/models - Get detailed model information\")\n",
    "    print(\"  POST /api/translate - Translate single text\")\n",
    "    print(\"  POST /api/batch-translate - Translate multiple texts\")\n",
    "    print(\"  POST /api/warmup - Warm up translators for better performance\")\n",
    "    print(\"  POST /api/clear-cache - Clear translator cache\")\n",
    "    \n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"# English to Hindi\")\n",
    "    print(\"curl -X POST http://localhost:5000/api/translate \\\\\")\n",
    "    print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "    print(\"  -d '{\\\"text\\\": \\\"Hello world\\\", \\\"src_lang\\\": \\\"en\\\", \\\"tgt_lang\\\": \\\"hi\\\"}'\")\n",
    "    \n",
    "    print(\"\\n# Hindi to English\")\n",
    "    print(\"curl -X POST http://localhost:5000/api/translate \\\\\")\n",
    "    print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "    print(\"  -d '{\\\"text\\\": \\\"नमस्ते दुनिया\\\", \\\"src_lang\\\": \\\"hi\\\", \\\"tgt_lang\\\": \\\"en\\\"}'\")\n",
    "    \n",
    "    print(\"\\n# Batch translation\")\n",
    "    print(\"curl -X POST http://localhost:5000/api/batch-translate \\\\\")\n",
    "    print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "    print(\"  -d '{\\\"texts\\\": [\\\"Hello\\\", \\\"Good morning\\\"], \\\"src_lang\\\": \\\"en\\\", \\\"tgt_lang\\\": \\\"hi\\\"}'\")\n",
    "    \n",
    "    return thread\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_flask_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt_rerun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
