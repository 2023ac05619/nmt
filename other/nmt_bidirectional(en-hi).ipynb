{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19beb840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training English-Hindi model...\n",
      "--- Starting Fine-Tuning MarianMT English-Hindi ---\n",
      "Loading Helsinki-NLP/opus-mt-en-hi...\n",
      "Loading samanantar dataset...\n",
      "Found 5000 valid translation pairs\n",
      "Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499a3887413d4236ae0d7a823e153c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c786b3ffbff14fbaaa775e7f94885c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting English-Hindi training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.168100</td>\n",
       "      <td>3.757415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.692200</td>\n",
       "      <td>3.588024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.359100</td>\n",
       "      <td>3.524310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.313500</td>\n",
       "      <td>3.505749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving English-Hindi model to results/marian_en_hi_finetuned\n",
      "Training Hindi-English model...\n",
      "--- Starting Fine-Tuning MarianMT Hindi-English ---\n",
      "Loading Helsinki-NLP/opus-mt-hi-en...\n",
      "Loading samanantar dataset...\n",
      "Found 5000 valid translation pairs\n",
      "Preprocessing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457663e17b4b4497b8f78a644924fca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16433c38b4ea43929ac6b33cf4393e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hindi-English training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.152300</td>\n",
       "      <td>3.755752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.610300</td>\n",
       "      <td>3.590309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.230600</td>\n",
       "      <td>3.539832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.208100</td>\n",
       "      <td>3.515731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Hindi-English model to results/marian_hi_en_finetuned\n",
      "\n",
      "--- Starting Evaluation ---\n",
      "\n",
      "--- Evaluating English-Hindi Translation ---\n",
      "Loading translator from: results/marian_en_hi_finetuned\n",
      "Detected model type: marian\n",
      "Translator ready!\n",
      " 1. 'Hello'\n",
      "    → सलाम\n",
      " 2. 'How are you?'\n",
      "    → आप कैसे हैं?\n",
      " 3. 'Good morning'\n",
      "    → सुप्रभात\n",
      " 4. 'Thank you'\n",
      "    → धन्यवाद\n",
      " 5. 'I am fine'\n",
      "    → मैं ठीक हूं\n",
      "\n",
      "================================================================================\n",
      "ENGLISH-HINDI EVALUATION RESULTS\n",
      "================================================================================\n",
      "      Source    Reference   Prediction   BLEU  METEOR\n",
      "       Hello       नमस्ते         सलाम   0.00    0.00\n",
      "How are you? आप कैसे हैं? आप कैसे हैं? 100.00   99.22\n",
      "Good morning     सुप्रभात     सुप्रभात  17.78   50.00\n",
      "   Thank you      धन्यवाद      धन्यवाद  17.78   50.00\n",
      "   I am fine  मैं ठीक हूं  मैं ठीक हूं  56.23   98.15\n",
      "\n",
      "Average BLEU: 47.95\n",
      "Average METEOR: 74.34\n",
      "Success rate: 4/5 (80.0%)\n",
      "\n",
      "--- Evaluating Hindi-English Translation ---\n",
      "Loading translator from: results/marian_hi_en_finetuned\n",
      "Detected model type: marian\n",
      "Translator ready!\n",
      " 1. 'नमस्ते'\n",
      "    → Hi.\n",
      " 2. 'आप कैसे हैं?'\n",
      "    → How are you?\n",
      " 3. 'सुप्रभात'\n",
      "    → Good morning\n",
      " 4. 'धन्यवाद'\n",
      "    → Thank you\n",
      " 5. 'मैं ठीक हूं'\n",
      "    → I'm fine\n",
      "\n",
      "================================================================================\n",
      "HINDI-ENGLISH EVALUATION RESULTS\n",
      "================================================================================\n",
      "      Source    Reference   Prediction   BLEU  METEOR\n",
      "      नमस्ते        Hello          Hi.   0.00   45.45\n",
      "आप कैसे हैं? How are you? How are you? 100.00   99.22\n",
      "    सुप्रभात Good morning Good morning  31.62   93.75\n",
      "     धन्यवाद    Thank you    Thank you  31.62   93.75\n",
      " मैं ठीक हूं    I am fine     I'm fine  13.51   33.33\n",
      "\n",
      "Average BLEU: 44.19\n",
      "Average METEOR: 80.01\n",
      "Success rate: 4/5 (80.0%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def prepare_translation_data():\n",
    "    \"\"\"Prepare translation data from samanantar dataset.\"\"\"\n",
    "    print(\"Loading samanantar dataset...\")\n",
    "    dataset = load_dataset(\"ai4bharat/samanantar\", \"hi\", split='train')\n",
    "    \n",
    "    # Filter valid pairs and create smaller dataset for testing\n",
    "    valid_pairs = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= 5000:  # Use smaller dataset for debugging\n",
    "            break\n",
    "        if example['src'] and example['tgt'] and len(example['src'].strip()) > 0 and len(example['tgt'].strip()) > 0:\n",
    "            valid_pairs.append({\n",
    "                'english': example['src'].strip(),\n",
    "                'hindi': example['tgt'].strip()\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(valid_pairs)} valid translation pairs\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_idx = int(0.9 * len(valid_pairs))\n",
    "    train_pairs = valid_pairs[:split_idx]\n",
    "    val_pairs = valid_pairs[split_idx:]\n",
    "    \n",
    "    return train_pairs, val_pairs\n",
    "\n",
    "def fine_tune_marian_en_hi():\n",
    "    \"\"\"Fine-tune MarianMT model for English-Hindi translation.\"\"\"\n",
    "    print(\"--- Starting Fine-Tuning MarianMT English-Hindi ---\")\n",
    "    \n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "    output_dir = \"results/marian_en_hi_finetuned\"\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Prepare data\n",
    "        train_pairs, val_pairs = prepare_translation_data()\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            inputs = examples[\"english\"]\n",
    "            targets = examples[\"hindi\"] \n",
    "            model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "            \n",
    "            # Setup the tokenizer for targets\n",
    "            labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        # Convert to datasets\n",
    "        train_dataset = Dataset.from_list(train_pairs)\n",
    "        val_dataset = Dataset.from_list(val_pairs)\n",
    "        \n",
    "        print(\"Preprocessing datasets...\")\n",
    "        train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            logging_steps=50,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=200,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting English-Hindi training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        print(f\"Saving English-Hindi model to {output_dir}\")\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MarianMT English-Hindi failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def fine_tune_marian_hi_en():\n",
    "    \"\"\"Fine-tune MarianMT model for Hindi-English translation.\"\"\"\n",
    "    print(\"--- Starting Fine-Tuning MarianMT Hindi-English ---\")\n",
    "    \n",
    "    model_name = \"Helsinki-NLP/opus-mt-hi-en\"\n",
    "    output_dir = \"results/marian_hi_en_finetuned\"\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        print(f\"Loading {model_name}...\")\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Prepare data (reversed for Hindi to English)\n",
    "        train_pairs, val_pairs = prepare_translation_data()\n",
    "        \n",
    "        # Reverse the pairs for Hindi-English translation\n",
    "        train_pairs_reversed = [{'english': pair['hindi'], 'hindi': pair['english']} for pair in train_pairs]\n",
    "        val_pairs_reversed = [{'english': pair['hindi'], 'hindi': pair['english']} for pair in val_pairs]\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            inputs = examples[\"english\"]  # Now contains Hindi text\n",
    "            targets = examples[\"hindi\"]   # Now contains English text\n",
    "            model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "            \n",
    "            # Setup the tokenizer for targets\n",
    "            labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        # Convert to datasets\n",
    "        train_dataset = Dataset.from_list(train_pairs_reversed)\n",
    "        val_dataset = Dataset.from_list(val_pairs_reversed)\n",
    "        \n",
    "        print(\"Preprocessing datasets...\")\n",
    "        train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            logging_steps=50,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=200,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting Hindi-English training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        print(f\"Saving Hindi-English model to {output_dir}\")\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MarianMT Hindi-English failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def fine_tune_t5_model():\n",
    "    \"\"\"Fallback: Fine-tune T5 model for bidirectional translation.\"\"\"\n",
    "    from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "    \n",
    "    model_name = \"t5-small\"\n",
    "    output_dir = \"results/t5_bidirectional_finetuned\"\n",
    "    \n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_pairs, val_pairs = prepare_translation_data()\n",
    "    \n",
    "    # Create bidirectional training data with T5 format\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    # English to Hindi\n",
    "    for pair in train_pairs:\n",
    "        train_data.append({\n",
    "            'input_text': f\"translate English to Hindi: {pair['english']}\",\n",
    "            'target_text': pair['hindi']\n",
    "        })\n",
    "        # Hindi to English (reverse)\n",
    "        train_data.append({\n",
    "            'input_text': f\"translate Hindi to English: {pair['hindi']}\",\n",
    "            'target_text': pair['english']\n",
    "        })\n",
    "    \n",
    "    for pair in val_pairs:\n",
    "        val_data.append({\n",
    "            'input_text': f\"translate English to Hindi: {pair['english']}\",\n",
    "            'target_text': pair['hindi']\n",
    "        })\n",
    "        val_data.append({\n",
    "            'input_text': f\"translate Hindi to English: {pair['hindi']}\",\n",
    "            'target_text': pair['english']\n",
    "        })\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[\"input_text\"]\n",
    "        targets = examples[\"target_text\"]\n",
    "        model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "        labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    # Convert to datasets\n",
    "    train_dataset = Dataset.from_list(train_data)\n",
    "    val_dataset = Dataset.from_list(val_data)\n",
    "    \n",
    "    print(\"Preprocessing datasets...\")\n",
    "    train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "    val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        logging_steps=50,\n",
    "        save_steps=200,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=200,\n",
    "        predict_with_generate=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting T5 bidirectional training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Saving T5 model to {output_dir}\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "class UniversalTranslator:\n",
    "    \"\"\"Universal translator that works with different model types.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        print(f\"Loading translator from: {model_path}\")\n",
    "        \n",
    "        # Detect model type\n",
    "        config_path = os.path.join(model_path, \"config.json\")\n",
    "        self.model_type = \"unknown\"\n",
    "        \n",
    "        if os.path.exists(config_path):\n",
    "            import json\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                if \"marian\" in config.get(\"architectures\", [\"\"])[0].lower():\n",
    "                    self.model_type = \"marian\"\n",
    "                elif \"t5\" in config.get(\"architectures\", [\"\"])[0].lower():\n",
    "                    self.model_type = \"t5\"\n",
    "        \n",
    "        print(f\"Detected model type: {self.model_type}\")\n",
    "        \n",
    "        if self.model_type == \"marian\":\n",
    "            self.tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "            self.model = MarianMTModel.from_pretrained(model_path)\n",
    "        else:  # Default to T5\n",
    "            from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "            self.model_type = \"t5\"\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"Translator ready!\")\n",
    "\n",
    "    def translate(self, text, src_lang='en', tgt_lang='hi'):\n",
    "        \"\"\"Translate text based on model type.\"\"\"\n",
    "        if self.model_type == \"marian\":\n",
    "            # MarianMT expects plain text\n",
    "            input_text = text\n",
    "        else:  # T5\n",
    "            lang_map = {'en': 'English', 'hi': 'Hindi'}\n",
    "            input_text = f\"translate {lang_map[src_lang]} to {lang_map[tgt_lang]}: {text}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                length_penalty=0.6,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation.strip()\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self):\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    def calculate_bleu(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(sentence_bleu([ref_tokens], cand_tokens, smoothing_function=self.smoothing) * 100, 2)\n",
    "    \n",
    "    def calculate_meteor(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(meteor_score([ref_tokens], cand_tokens) * 100, 2)\n",
    "\n",
    "def run_evaluation(model_paths):\n",
    "    \"\"\"Evaluate the trained models.\"\"\"\n",
    "    print(\"\\n--- Starting Evaluation ---\")\n",
    "    \n",
    "    # Download NLTK data\n",
    "    for corpus in ['punkt', 'wordnet', 'omw-1.4']:\n",
    "        nltk.download(corpus, quiet=True)\n",
    "\n",
    "    evaluator = TranslationEvaluator()\n",
    "    \n",
    "    # Test cases for both directions\n",
    "    en_to_hi_cases = [\n",
    "        {'source': 'Hello', 'reference': 'नमस्ते'},\n",
    "        {'source': 'How are you?', 'reference': 'आप कैसे हैं?'},\n",
    "        {'source': 'Good morning', 'reference': 'सुप्रभात'},\n",
    "        {'source': 'Thank you', 'reference': 'धन्यवाद'},\n",
    "        {'source': 'I am fine', 'reference': 'मैं ठीक हूं'},\n",
    "    ]\n",
    "    \n",
    "    hi_to_en_cases = [\n",
    "        {'source': 'नमस्ते', 'reference': 'Hello'},\n",
    "        {'source': 'आप कैसे हैं?', 'reference': 'How are you?'},\n",
    "        {'source': 'सुप्रभात', 'reference': 'Good morning'},\n",
    "        {'source': 'धन्यवाद', 'reference': 'Thank you'},\n",
    "        {'source': 'मैं ठीक हूं', 'reference': 'I am fine'},\n",
    "    ]\n",
    "    \n",
    "    # Evaluate both directions\n",
    "    for direction, cases, model_path in [\n",
    "        (\"English-Hindi\", en_to_hi_cases, model_paths.get('en_hi')),\n",
    "        (\"Hindi-English\", hi_to_en_cases, model_paths.get('hi_en'))\n",
    "    ]:\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            print(f\"Model not found for {direction}: {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- Evaluating {direction} Translation ---\")\n",
    "        translator = UniversalTranslator(model_path)\n",
    "        results = []\n",
    "        \n",
    "        for i, case in enumerate(cases, 1):\n",
    "            print(f\"{i:2d}. '{case['source']}'\")\n",
    "            \n",
    "            try:\n",
    "                src_lang = 'en' if direction == \"English-Hindi\" else 'hi'\n",
    "                tgt_lang = 'hi' if direction == \"English-Hindi\" else 'en'\n",
    "                \n",
    "                prediction = translator.translate(case['source'], src_lang, tgt_lang)\n",
    "                bleu = evaluator.calculate_bleu(case['reference'], prediction)\n",
    "                meteor = evaluator.calculate_meteor(case['reference'], prediction)\n",
    "                \n",
    "                results.append({\n",
    "                    'Source': case['source'],\n",
    "                    'Reference': case['reference'], \n",
    "                    'Prediction': prediction,\n",
    "                    'BLEU': bleu,\n",
    "                    'METEOR': meteor\n",
    "                })\n",
    "                \n",
    "                print(f\"    → {prediction}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    → ERROR: {str(e)}\")\n",
    "                results.append({\n",
    "                    'Source': case['source'],\n",
    "                    'Reference': case['reference'],\n",
    "                    'Prediction': f\"ERROR: {str(e)}\",\n",
    "                    'BLEU': 0.0,\n",
    "                    'METEOR': 0.0\n",
    "                })\n",
    "        \n",
    "        # Results summary\n",
    "        df = pd.DataFrame(results)\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{direction.upper()} EVALUATION RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(df.to_string(index=False, max_colwidth=40))\n",
    "        \n",
    "        valid_results = df[df['BLEU'] > 0]\n",
    "        if len(valid_results) > 0:\n",
    "            avg_bleu = valid_results['BLEU'].mean()\n",
    "            avg_meteor = valid_results['METEOR'].mean()\n",
    "            print(f\"\\nAverage BLEU: {avg_bleu:.2f}\")\n",
    "            print(f\"Average METEOR: {avg_meteor:.2f}\")\n",
    "            print(f\"Success rate: {len(valid_results)}/{len(results)} ({100*len(valid_results)/len(results):.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\nNo successful translations generated.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    model_paths = {}\n",
    "    \n",
    "    # Train English-Hindi model\n",
    "    print(\"Training English-Hindi model...\")\n",
    "    en_hi_path = fine_tune_marian_en_hi()\n",
    "    if en_hi_path:\n",
    "        model_paths['en_hi'] = en_hi_path\n",
    "    \n",
    "    # Train Hindi-English model\n",
    "    print(\"Training Hindi-English model...\")\n",
    "    hi_en_path = fine_tune_marian_hi_en()\n",
    "    if hi_en_path:\n",
    "        model_paths['hi_en'] = hi_en_path\n",
    "    \n",
    "    # If both MarianMT models fail, train T5 as fallback\n",
    "    if not model_paths:\n",
    "        print(\"Both MarianMT models failed. Training T5 fallback...\")\n",
    "        t5_path = fine_tune_t5_model()\n",
    "        if t5_path:\n",
    "            model_paths['en_hi'] = t5_path\n",
    "            model_paths['hi_en'] = t5_path\n",
    "    \n",
    "    # Evaluate models\n",
    "    if model_paths:\n",
    "        run_evaluation(model_paths)\n",
    "    else:\n",
    "        print(\"No models were successfully trained.\")\n",
    "    \n",
    "    return model_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7161b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing translators...\n",
      "Loading marian model from: results/marian_en_hi_finetuned\n",
      "Model loaded successfully: marian\n",
      "Cached translator for en -> hi\n",
      "✓ Initialized en -> hi translator\n",
      "Loading marian model from: results/marian_hi_en_finetuned\n",
      "Model loaded successfully: marian\n",
      "Cached translator for hi -> en\n",
      "✓ Initialized hi -> en translator\n",
      "Translator initialization complete.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.202.209:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [19/Aug/2025 09:24:37] \"OPTIONS /api/translate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [19/Aug/2025 09:24:37] \"POST /api/translate HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import time\n",
    "import threading\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from transformers import MarianMTModel, MarianTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Global translator cache and configuration\n",
    "translator_cache = {}\n",
    "LANGUAGES = {'en': 'English', 'hi': 'Hindi'}\n",
    "MODEL_PATHS = {\n",
    "    'en_hi': 'results/marian_en_hi_finetuned',\n",
    "    'hi_en': 'results/marian_hi_en_finetuned',\n",
    "    'fallback': 'results/t5_bidirectional_finetuned'\n",
    "}\n",
    "\n",
    "class DynamicTranslator:\n",
    "    \"\"\"Dynamic translator that loads models on demand based on language pair.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.model_type = self._detect_model_type()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._load_model()\n",
    "    \n",
    "    def _detect_model_type(self):\n",
    "        \"\"\"Detect model type from config.json\"\"\"\n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        \n",
    "        if os.path.exists(config_path):\n",
    "            try:\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config = json.load(f)\n",
    "                    architecture = config.get(\"architectures\", [\"\"])[0].lower()\n",
    "                    if \"marian\" in architecture:\n",
    "                        return \"marian\"\n",
    "                    elif \"t5\" in architecture:\n",
    "                        return \"t5\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading config: {e}\")\n",
    "        \n",
    "        # Fallback detection based on path\n",
    "        if \"marian\" in self.model_path.lower():\n",
    "            return \"marian\"\n",
    "        elif \"t5\" in self.model_path.lower():\n",
    "            return \"t5\"\n",
    "        \n",
    "        return \"unknown\"\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model and tokenizer based on detected type.\"\"\"\n",
    "        print(f\"Loading {self.model_type} model from: {self.model_path}\")\n",
    "        \n",
    "        try:\n",
    "            if self.model_type == \"marian\":\n",
    "                self.tokenizer = MarianTokenizer.from_pretrained(self.model_path)\n",
    "                self.model = MarianMTModel.from_pretrained(self.model_path)\n",
    "            else:  # T5 or unknown (default to T5)\n",
    "                self.tokenizer = T5Tokenizer.from_pretrained(self.model_path)\n",
    "                self.model = T5ForConditionalGeneration.from_pretrained(self.model_path)\n",
    "                self.model_type = \"t5\"\n",
    "            \n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(f\"Model loaded successfully: {self.model_type}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def translate(self, text, src_lang='en', tgt_lang='hi'):\n",
    "        \"\"\"Translate text based on model type.\"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise Exception(\"Model not loaded\")\n",
    "        \n",
    "        # Prepare input based on model type\n",
    "        if self.model_type == \"marian\":\n",
    "            input_text = text\n",
    "        else:  # T5\n",
    "            lang_map = {'en': 'English', 'hi': 'Hindi'}\n",
    "            input_text = f\"translate {lang_map[src_lang]} to {lang_map[tgt_lang]}: {text}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                length_penalty=0.6,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        # Decode and return\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation.strip()\n",
    "\n",
    "def get_model_path(src_lang, tgt_lang):\n",
    "    \"\"\"Get appropriate model path based on language pair.\"\"\"\n",
    "    language_pair = f\"{src_lang}_{tgt_lang}\"\n",
    "    \n",
    "    # Check if specific directional model exists\n",
    "    if language_pair in MODEL_PATHS and os.path.exists(MODEL_PATHS[language_pair]):\n",
    "        return MODEL_PATHS[language_pair]\n",
    "    \n",
    "    # Check if fallback model exists\n",
    "    if os.path.exists(MODEL_PATHS['fallback']):\n",
    "        return MODEL_PATHS['fallback']\n",
    "    \n",
    "    # Try to find any available model\n",
    "    for path in MODEL_PATHS.values():\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_translator(src_lang, tgt_lang):\n",
    "    \"\"\"Get or create translator for specific language pair.\"\"\"\n",
    "    cache_key = f\"{src_lang}_{tgt_lang}\"\n",
    "    \n",
    "    # Return cached translator if available\n",
    "    if cache_key in translator_cache:\n",
    "        return translator_cache[cache_key]\n",
    "    \n",
    "    # Get model path\n",
    "    model_path = get_model_path(src_lang, tgt_lang)\n",
    "    \n",
    "    if not model_path:\n",
    "        raise Exception(f\"No model available for {src_lang} -> {tgt_lang} translation\")\n",
    "    \n",
    "    # Create and cache new translator\n",
    "    try:\n",
    "        translator = DynamicTranslator(model_path)\n",
    "        translator_cache[cache_key] = translator\n",
    "        print(f\"Cached translator for {src_lang} -> {tgt_lang}\")\n",
    "        return translator\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load translator: {str(e)}\")\n",
    "\n",
    "def initialize_translators():\n",
    "    \"\"\"Pre-initialize common translators.\"\"\"\n",
    "    print(\"Initializing translators...\")\n",
    "    \n",
    "    common_pairs = [('en', 'hi'), ('hi', 'en')]\n",
    "    \n",
    "    for src_lang, tgt_lang in common_pairs:\n",
    "        try:\n",
    "            get_translator(src_lang, tgt_lang)\n",
    "            print(f\"✓ Initialized {src_lang} -> {tgt_lang} translator\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to initialize {src_lang} -> {tgt_lang}: {e}\")\n",
    "    \n",
    "    print(\"Translator initialization complete.\")\n",
    "\n",
    "@app.route('/api/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint with detailed model status.\"\"\"\n",
    "    model_status = {}\n",
    "    \n",
    "    for pair, path in MODEL_PATHS.items():\n",
    "        model_status[pair] = {\n",
    "            'path': path,\n",
    "            'exists': os.path.exists(path),\n",
    "            'cached': f\"{pair.replace('_', '_')}\" in translator_cache if '_' in pair else False\n",
    "        }\n",
    "    \n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'supported_languages': LANGUAGES,\n",
    "        'model_status': model_status,\n",
    "        'cached_translators': list(translator_cache.keys()),\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    })\n",
    "\n",
    "@app.route('/api/translate', methods=['POST'])\n",
    "def translate_text():\n",
    "    \"\"\"Single text translation endpoint.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        text = data.get('text', '').strip()\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "\n",
    "        # Validation\n",
    "        if not text:\n",
    "            return jsonify({'error': 'No text provided for translation.'}), 400\n",
    "        if src_lang not in LANGUAGES or tgt_lang not in LANGUAGES:\n",
    "            return jsonify({'error': 'Unsupported language selected.'}), 400\n",
    "        if src_lang == tgt_lang:\n",
    "            return jsonify({'error': 'Source and target languages are the same.'}), 400\n",
    "\n",
    "        # Get translator and translate\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            translator = get_translator(src_lang, tgt_lang)\n",
    "            translation = translator.translate(text, src_lang, tgt_lang)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': f'Translation failed: {str(e)}'}), 503\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        return jsonify({\n",
    "            'source_text': text,\n",
    "            'source_language': src_lang,\n",
    "            'source_language_name': LANGUAGES[src_lang],\n",
    "            'target_language': tgt_lang,\n",
    "            'target_language_name': LANGUAGES[tgt_lang],\n",
    "            'translation': translation,\n",
    "            'model_used': get_model_path(src_lang, tgt_lang),\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/batch-translate', methods=['POST'])\n",
    "def batch_translate():\n",
    "    \"\"\"Batch translation endpoint.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        texts = data.get('texts', [])\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "        \n",
    "        # Validation\n",
    "        if not texts or not isinstance(texts, list):\n",
    "            return jsonify({'error': 'No texts array provided.'}), 400\n",
    "        if len(texts) > 100:\n",
    "            return jsonify({'error': 'Maximum 100 texts allowed per batch.'}), 400\n",
    "        if src_lang not in LANGUAGES or tgt_lang not in LANGUAGES:\n",
    "            return jsonify({'error': 'Unsupported language selected.'}), 400\n",
    "        if src_lang == tgt_lang:\n",
    "            return jsonify({'error': 'Source and target languages are the same.'}), 400\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get translator\n",
    "        try:\n",
    "            translator = get_translator(src_lang, tgt_lang)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': f'Failed to load translator: {str(e)}'}), 503\n",
    "        \n",
    "        translations = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if isinstance(text, str) and text.strip():\n",
    "                try:\n",
    "                    translation = translator.translate(text.strip(), src_lang, tgt_lang)\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': translation,\n",
    "                        'success': True\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': None,\n",
    "                        'success': False,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "            else:\n",
    "                translations.append({\n",
    "                    'source': text,\n",
    "                    'translation': None,\n",
    "                    'success': False,\n",
    "                    'error': 'Invalid text format'\n",
    "                })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        return jsonify({\n",
    "            'translations': translations,\n",
    "            'source_language': src_lang,\n",
    "            'target_language': tgt_lang,\n",
    "            'model_used': get_model_path(src_lang, tgt_lang),\n",
    "            'total_count': len(translations),\n",
    "            'success_count': sum(1 for t in translations if t['success']),\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Batch translation error: {e}\")\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/languages', methods=['GET'])\n",
    "def get_languages():\n",
    "    \"\"\"Get supported languages and available translation pairs.\"\"\"\n",
    "    available_pairs = []\n",
    "    \n",
    "    # Check which models are actually available\n",
    "    for src in LANGUAGES.keys():\n",
    "        for tgt in LANGUAGES.keys():\n",
    "            if src != tgt:\n",
    "                model_path = get_model_path(src, tgt)\n",
    "                if model_path:\n",
    "                    available_pairs.append({\n",
    "                        'source': src,\n",
    "                        'target': tgt,\n",
    "                        'source_name': LANGUAGES[src],\n",
    "                        'target_name': LANGUAGES[tgt],\n",
    "                        'model_path': model_path\n",
    "                    })\n",
    "    \n",
    "    return jsonify({\n",
    "        'supported_languages': LANGUAGES,\n",
    "        'available_translation_pairs': available_pairs,\n",
    "        'total_pairs': len(available_pairs)\n",
    "    })\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "def get_models():\n",
    "    \"\"\"Get information about available models.\"\"\"\n",
    "    models_info = {}\n",
    "    \n",
    "    for model_key, path in MODEL_PATHS.items():\n",
    "        model_info = {\n",
    "            'path': path,\n",
    "            'exists': os.path.exists(path),\n",
    "            'type': 'unknown',\n",
    "            'cached': False\n",
    "        }\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            # Detect model type\n",
    "            config_path = os.path.join(path, \"config.json\")\n",
    "            if os.path.exists(config_path):\n",
    "                try:\n",
    "                    with open(config_path, 'r') as f:\n",
    "                        config = json.load(f)\n",
    "                        architecture = config.get(\"architectures\", [\"\"])[0].lower()\n",
    "                        if \"marian\" in architecture:\n",
    "                            model_info['type'] = \"marian\"\n",
    "                        elif \"t5\" in architecture:\n",
    "                            model_info['type'] = \"t5\"\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Check if cached\n",
    "            cache_key = model_key.replace('_', '_')\n",
    "            model_info['cached'] = cache_key in translator_cache\n",
    "        \n",
    "        models_info[model_key] = model_info\n",
    "    \n",
    "    return jsonify({\n",
    "        'models': models_info,\n",
    "        'cache_status': list(translator_cache.keys())\n",
    "    })\n",
    "\n",
    "@app.route('/api/clear-cache', methods=['POST'])\n",
    "def clear_cache():\n",
    "    \"\"\"Clear translator cache.\"\"\"\n",
    "    global translator_cache\n",
    "    \n",
    "    # Get current cache size\n",
    "    cache_size = len(translator_cache)\n",
    "    \n",
    "    # Clear cache\n",
    "    translator_cache.clear()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return jsonify({\n",
    "        'message': 'Cache cleared successfully',\n",
    "        'cleared_translators': cache_size,\n",
    "        'current_cache_size': len(translator_cache)\n",
    "    })\n",
    "\n",
    "@app.route('/api/warmup', methods=['POST'])\n",
    "def warmup_translators():\n",
    "    \"\"\"Warm up translators for better performance.\"\"\"\n",
    "    data = request.json or {}\n",
    "    language_pairs = data.get('pairs', [('en', 'hi'), ('hi', 'en')])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for src_lang, tgt_lang in language_pairs:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            translator = get_translator(src_lang, tgt_lang)\n",
    "            \n",
    "            # Perform a dummy translation to warm up\n",
    "            test_text = \"Hello\" if src_lang == 'en' else \"नमस्ते\"\n",
    "            translator.translate(test_text, src_lang, tgt_lang)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            results.append({\n",
    "                'pair': f\"{src_lang} -> {tgt_lang}\",\n",
    "                'success': True,\n",
    "                'warmup_time': round(end_time - start_time, 3)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'pair': f\"{src_lang} -> {tgt_lang}\",\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return jsonify({\n",
    "        'message': 'Warmup completed',\n",
    "        'results': results,\n",
    "        'total_cached': len(translator_cache)\n",
    "    })\n",
    "\n",
    "def run_flask_app():\n",
    "    \"\"\"Run Flask application.\"\"\"\n",
    "    initialize_translators()\n",
    "    app.run(debug=False, use_reloader=False, host='0.0.0.0', port=5000, threaded=True)\n",
    "\n",
    "def start_api_server():\n",
    "    \"\"\"Start API server in a thread.\"\"\"\n",
    "    thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "    thread.start()\n",
    "    return thread\n",
    "\n",
    "def start_translation_api():\n",
    "    \"\"\"Start translation API for Jupyter notebook usage.\"\"\"\n",
    "    print(\"Starting enhanced translation API server...\")\n",
    "    thread = start_api_server()\n",
    "    print(\"API server started on http://localhost:5000\")\n",
    "    print(\"\\nAvailable endpoints:\")\n",
    "    print(\"  GET  /api/health - Check server status and model availability\")\n",
    "    print(\"  GET  /api/languages - Get supported languages and available pairs\")\n",
    "    print(\"  GET  /api/models - Get detailed model information\")\n",
    "    print(\"  POST /api/translate - Translate single text\")\n",
    "    print(\"  POST /api/batch-translate - Translate multiple texts\")\n",
    "    print(\"  POST /api/warmup - Warm up translators for better performance\")\n",
    "    print(\"  POST /api/clear-cache - Clear translator cache\")\n",
    "    \n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"# English to Hindi\")\n",
    "    print(\"curl -X POST http://localhost:5000/api/translate \\\\\")\n",
    "    print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "    print(\"  -d '{\\\"text\\\": \\\"Hello world\\\", \\\"src_lang\\\": \\\"en\\\", \\\"tgt_lang\\\": \\\"hi\\\"}'\")\n",
    "    \n",
    "    print(\"\\n# Hindi to English\")\n",
    "    print(\"curl -X POST http://localhost:5000/api/translate \\\\\")\n",
    "    print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "    print(\"  -d '{\\\"text\\\": \\\"नमस्ते दुनिया\\\", \\\"src_lang\\\": \\\"hi\\\", \\\"tgt_lang\\\": \\\"en\\\"}'\")\n",
    "    \n",
    "    print(\"\\n# Batch translation\")\n",
    "    print(\"curl -X POST http://localhost:5000/api/batch-translate \\\\\")\n",
    "    print(\"  -H 'Content-Type: application/json' \\\\\")\n",
    "    print(\"  -d '{\\\"texts\\\": [\\\"Hello\\\", \\\"Good morning\\\"], \\\"src_lang\\\": \\\"en\\\", \\\"tgt_lang\\\": \\\"hi\\\"}'\")\n",
    "    \n",
    "    return thread\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_flask_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt_rerun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
