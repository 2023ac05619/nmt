{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee96eaf",
   "metadata": {},
   "source": [
    "## Neural Machine Translation (NMT) application:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02ffb6",
   "metadata": {},
   "source": [
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "\n",
    "**Group ID:** 8\n",
    "\n",
    "**Group Members Name with Student ID:**\n",
    "\n",
    "| Sl.              | Student Name  | ID |\n",
    "| :---- | :------: | ----: |\n",
    "| 1. |  Imran Khan   | 2023ac05619 |\n",
    "| 2. |  Priya M   | 2023AC05056 |\n",
    "| 3. |  Mandar Khollam   | 2023AC05073 |\n",
    "| 4. |  Ketan Bharat Purohit   | 2023AD05062 |\n",
    "| 5. |  Nilesh Narayan Sonwane | 2023AC05827 |\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a614be4",
   "metadata": {},
   "source": [
    "### Setup for Fine-Tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233a7920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae76b9bb",
   "metadata": {},
   "source": [
    "###  Prepare translation data from ai4bharat/samanantar datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "134f8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Functions\n",
    "def prepare_translation_data(language=\"hi\"):\n",
    "    print(f\"Loading samanantar dataset for {language}...\")\n",
    "    dataset = load_dataset(\"ai4bharat/samanantar\", language, split='train')\n",
    "    \n",
    "    valid_pairs = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        if i >= 5000: # smaller dataset for debugging\n",
    "            break\n",
    "        # Filter valid pairs\n",
    "        if example['src'] and example['tgt'] and len(example['src'].strip()) > 0 and len(example['tgt'].strip()) > 0:\n",
    "            valid_pairs.append({\n",
    "                'english': example['src'].strip(),\n",
    "                'target': example['tgt'].strip()\n",
    "            })\n",
    "    \n",
    "    print(f\"Found {len(valid_pairs)} valid translation pairs\")\n",
    "    \n",
    "    # Split into train and validation\n",
    "    split_idx = int(0.9 * len(valid_pairs))\n",
    "    train_pairs = valid_pairs[:split_idx]\n",
    "    val_pairs = valid_pairs[split_idx:]\n",
    "    \n",
    "    return train_pairs, val_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdab0da",
   "metadata": {},
   "source": [
    "### Fine-tune MarianMT model for English-Hindi translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1c6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning MarianMT model for English-Hindi\n",
    "def fine_tune_marian_en_hi():\n",
    "    print(\"===== Starting Fine-Tuning MarianMT English-Hindi =====\")\n",
    "    # Load MarianMT model\n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "    # Output directory\n",
    "    output_dir = \"results/marian_en_hi_finetuned\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        # Prepare data\n",
    "        train_pairs, val_pairs = prepare_translation_data(\"hi\")\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            inputs = examples[\"english\"]\n",
    "            targets = examples[\"target\"] \n",
    "            model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "            labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        train_dataset = Dataset.from_list(train_pairs)\n",
    "        val_dataset = Dataset.from_list(val_pairs)\n",
    "        # Preprocess\n",
    "        train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "        # Data Collator\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "        # Training Arguments\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            logging_steps=50,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=200,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "        # Trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        # Train\n",
    "        trainer.train()\n",
    "        # Save model\n",
    "        trainer.save_model(output_dir)\n",
    "        # Save tokenizer\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MarianMT English-Hindi failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c17d08",
   "metadata": {},
   "source": [
    "### Fine-tune MarianMT model for Hindi-English translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99cfff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning MarianMT model for Hindi-English\n",
    "def fine_tune_marian_hi_en():\n",
    "    print(\"===== Starting Fine-Tuning MarianMT Hindi-English =====\")\n",
    "    # Load MarianMT model\n",
    "    model_name = \"Helsinki-NLP/opus-mt-hi-en\"\n",
    "    output_dir = \"results/marian_hi_en_finetuned\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        \n",
    "        train_pairs, val_pairs = prepare_translation_data(\"hi\")\n",
    "        \n",
    "        train_pairs_reversed = [{'english': pair['target'], 'target': pair['english']} for pair in train_pairs]\n",
    "        val_pairs_reversed = [{'english': pair['target'], 'target': pair['english']} for pair in val_pairs]\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            inputs = examples[\"english\"]\n",
    "            targets = examples[\"target\"]\n",
    "            model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "            labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        train_dataset = Dataset.from_list(train_pairs_reversed)\n",
    "        val_dataset = Dataset.from_list(val_pairs_reversed)\n",
    "        \n",
    "        train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "        \n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            logging_steps=50,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=200,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "        \n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MarianMT Hindi-English failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b99e6",
   "metadata": {},
   "source": [
    "### Fine-tune MarianMT model for English-Kannada translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dafb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning MarianMT model for English-Kannada\n",
    "def fine_tune_marian_en_kn():\n",
    "    \n",
    "    print(\"===== Starting Fine-Tuning MarianMT English-Kannada =====\")\n",
    "    \n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "    output_dir = \"results/marian_en_kn_finetuned\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        \n",
    "        train_pairs, val_pairs = prepare_translation_data(\"kn\")\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            inputs = examples[\"english\"]\n",
    "            targets = examples[\"target\"] \n",
    "            model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "            labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        train_dataset = Dataset.from_list(train_pairs)\n",
    "        val_dataset = Dataset.from_list(val_pairs)\n",
    "        \n",
    "        train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "        \n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            logging_steps=50,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=200,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "        \n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MarianMT English-Kannada failed: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a2b50",
   "metadata": {},
   "source": [
    "### Fine-tune MarianMT model for Kannada-English translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a9e537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tuning MarianMT model for Kannada-English\n",
    "def fine_tune_marian_kn_en():\n",
    "    print(\"===== Starting Fine-Tuning MarianMT Kannada-English =====\")\n",
    "    \n",
    "    model_name = \"Helsinki-NLP/opus-mt-mul-en\"\n",
    "    output_dir = \"results/marian_kn_en_finetuned\"\n",
    "    \n",
    "    try:\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "        \n",
    "        train_pairs, val_pairs = prepare_translation_data(\"kn\")\n",
    "        \n",
    "        train_pairs_reversed = [{'english': pair['target'], 'target': pair['english']} for pair in train_pairs]\n",
    "        val_pairs_reversed = [{'english': pair['target'], 'target': pair['english']} for pair in val_pairs]\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            inputs = examples[\"english\"]\n",
    "            targets = examples[\"target\"]\n",
    "            model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=False)\n",
    "            labels = tokenizer(text_target=targets, max_length=128, truncation=True, padding=False)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "        \n",
    "        train_dataset = Dataset.from_list(train_pairs_reversed)\n",
    "        val_dataset = Dataset.from_list(val_pairs_reversed)\n",
    "        \n",
    "        train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "        \n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "        \n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=200,\n",
    "            logging_steps=50,\n",
    "            save_steps=200,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=3e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=200,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "        \n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        trainer.save_model(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        return output_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MarianMT Kannada-English failed: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81277987",
   "metadata": {},
   "source": [
    "### Define a Universal Translator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98b64b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal Translator Class for all model types\n",
    "class UniversalTranslator:\n",
    "    # Initialize the translator\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.model_type = self._detect_model_type()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._load_model()\n",
    "    # Detect the model type\n",
    "    def _detect_model_type(self):\n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        \n",
    "        if os.path.exists(config_path):\n",
    "            try:\n",
    "                with open(config_path, 'r') as f:\n",
    "                    config = json.load(f)\n",
    "                    architecture = config.get(\"architectures\", [\"\"])[0].lower()\n",
    "                    if \"marian\" in architecture:\n",
    "                        return \"marian\"\n",
    "                    elif \"t5\" in architecture:\n",
    "                        return \"t5\"\n",
    "            except:\n",
    "                pass # Ignore errors\n",
    "            \n",
    "        # If config file doesn't exist, check model name\n",
    "        if \"marian\" in self.model_path.lower():\n",
    "            return \"marian\"\n",
    "        elif \"t5\" in self.model_path.lower():\n",
    "            return \"t5\"\n",
    "        # Default to T5\n",
    "        return \"t5\"\n",
    "    \n",
    "    # Load the model\n",
    "    def _load_model(self):\n",
    "        if self.model_type == \"marian\":\n",
    "            self.tokenizer = MarianTokenizer.from_pretrained(self.model_path)\n",
    "            self.model = MarianMTModel.from_pretrained(self.model_path)\n",
    "        else:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.model_path)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(self.model_path)\n",
    "            self.model_type = \"t5\"\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    # Translate text\n",
    "    def translate(self, text, src_lang='en', tgt_lang='hi'):\n",
    "        if self.model_type == \"marian\":\n",
    "            input_text = text\n",
    "        else:\n",
    "            lang_map = {'en': 'English', 'hi': 'Hindi', 'kn': 'Kannada'}\n",
    "            input_text = f\"translate {lang_map[src_lang]} to {lang_map[tgt_lang]}: {text}\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                length_penalty=0.6,\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        # Decode and return\n",
    "        translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return translation.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763c68c",
   "metadata": {},
   "source": [
    "### Translation Evaluation with METEOR and BLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787d61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator Class\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self):\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "    # Calculate BLEU\n",
    "    def calculate_bleu(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(sentence_bleu([ref_tokens], cand_tokens, smoothing_function=self.smoothing) * 100, 2)\n",
    "    # Calculate METEOR\n",
    "    def calculate_meteor(self, reference, candidate):\n",
    "        if not reference.strip() or not candidate.strip():\n",
    "            return 0.0\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        return round(meteor_score([ref_tokens], cand_tokens) * 100, 2)\n",
    "    \n",
    "# Run Evaluation\n",
    "def run_evaluation(model_paths):\n",
    "    \"\"\"Evaluate the trained models.\"\"\"\n",
    "    for corpus in ['punkt', 'wordnet', 'omw-1.4']:\n",
    "        nltk.download(corpus, quiet=True)\n",
    "    # Initialize evaluator    \n",
    "    evaluator = TranslationEvaluator()\n",
    "    \n",
    "    # Test cases for both directions\n",
    "    test_cases = {\n",
    "        'en_hi': [\n",
    "            {'source': 'Hello', 'reference': 'नमस्ते'},\n",
    "            {'source': 'How are you?', 'reference': 'आप कैसे हैं?'},\n",
    "            {'source': 'Good morning', 'reference': 'सुप्रभात'},\n",
    "            {'source': 'Thank you', 'reference': 'धन्यवाद'},\n",
    "            {'source': 'I am fine', 'reference': 'मैं ठीक हूं'},\n",
    "        ],\n",
    "        'hi_en': [\n",
    "            {'source': 'नमस्ते', 'reference': 'Hello'},\n",
    "            {'source': 'आप कैसे हैं?', 'reference': 'How are you?'},\n",
    "            {'source': 'सुप्रभात', 'reference': 'Good morning'},\n",
    "            {'source': 'धन्यवाद', 'reference': 'Thank you'},\n",
    "            {'source': 'मैं ठीक हूं', 'reference': 'I am fine'},\n",
    "        ],\n",
    "        'en_kn': [\n",
    "            {'source': 'Hello', 'reference': 'ನಮಸ್ಕಾರ'},\n",
    "            {'source': 'How are you?', 'reference': 'ಹೇಗಿದ್ದೀರಾ?'},\n",
    "            {'source': 'Good morning', 'reference': 'ಶುಭೋದಯ'},\n",
    "            {'source': 'Thank you', 'reference': 'ಧನ್ಯವಾದಗಳು'},\n",
    "            {'source': 'I am fine', 'reference': 'ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ'},\n",
    "        ],\n",
    "        'kn_en': [\n",
    "            {'source': 'ನಮಸ್ಕಾರ', 'reference': 'Hello'},\n",
    "            {'source': 'ಹೇಗಿದ್ದೀರಾ?', 'reference': 'How are you?'},\n",
    "            {'source': 'ಶುಭೋದಯ', 'reference': 'Good morning'},\n",
    "            {'source': 'ಧನ್ಯವಾದಗಳು', 'reference': 'Thank you'},\n",
    "            {'source': 'ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ', 'reference': 'I am fine'},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for direction, cases in test_cases.items():\n",
    "        model_path = model_paths.get(direction)\n",
    "        if not model_path or not os.path.exists(model_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n===== Evaluating {direction.upper()} Translation =====\")\n",
    "        translator = UniversalTranslator(model_path)\n",
    "        results = []\n",
    "        \n",
    "        src_lang, tgt_lang = direction.split('_')\n",
    "        for i, case in enumerate(cases, 1):\n",
    "            try:\n",
    "                prediction = translator.translate(case['source'], src_lang, tgt_lang)\n",
    "                bleu = evaluator.calculate_bleu(case['reference'], prediction)\n",
    "                meteor = evaluator.calculate_meteor(case['reference'], prediction)\n",
    "                \n",
    "                results.append({\n",
    "                    'Source': case['source'],\n",
    "                    'Reference': case['reference'], \n",
    "                    'Prediction': prediction,\n",
    "                    'BLEU': bleu,\n",
    "                    'METEOR': meteor\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'Source': case['source'],\n",
    "                    'Reference': case['reference'],\n",
    "                    'Prediction': f\"ERROR: {str(e)}\",\n",
    "                    'BLEU': 0.0,\n",
    "                    'METEOR': 0.0\n",
    "                })\n",
    "        \n",
    "        # Data Frame to format the results\n",
    "        df = pd.DataFrame(results)\n",
    "        print(df.to_string(index=False, max_colwidth=40))\n",
    "        \n",
    "        valid_results = df[df['BLEU'] > 0]\n",
    "        if len(valid_results) > 0:\n",
    "            avg_bleu = valid_results['BLEU'].mean()\n",
    "            avg_meteor = valid_results['METEOR'].mean()\n",
    "            print(f\"\\nAverage BLEU: {avg_bleu:.2f}\")\n",
    "            print(f\"Average METEOR: {avg_meteor:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9118865",
   "metadata": {},
   "source": [
    "### Start Fine-tunning processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94383365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Starting Fine-Tuning MarianMT English-Hindi =====\n",
      "Loading samanantar dataset for hi...\n",
      "Found 5000 valid translation pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c518b38e1f4d29af3fce6b9f577a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e22af71f714a3294374ee5ea6b29b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.168100</td>\n",
       "      <td>3.757415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.692200</td>\n",
       "      <td>3.588024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.359100</td>\n",
       "      <td>3.524310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.313500</td>\n",
       "      <td>3.505749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Starting Fine-Tuning MarianMT Hindi-English =====\n",
      "Loading samanantar dataset for hi...\n",
      "Found 5000 valid translation pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea06a6bbf37455cb87cd38d53b3bb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408f6b4655a2478fac46c81d1c000ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.152300</td>\n",
       "      <td>3.755752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.610300</td>\n",
       "      <td>3.590309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.230600</td>\n",
       "      <td>3.539832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.208100</td>\n",
       "      <td>3.515731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n",
      "Checkpoint destination directory results/marian_hi_en_finetuned/checkpoint-800 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[61126]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Starting Fine-Tuning MarianMT English-Kannada =====\n",
      "Loading samanantar dataset for kn...\n",
      "Found 5000 valid translation pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a421e7632d4d088b15933dcab4ae1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3150d699ac1741d2a2ded7eb414b7c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.461700</td>\n",
       "      <td>2.213240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.158300</td>\n",
       "      <td>2.078920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.956900</td>\n",
       "      <td>2.021572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.960800</td>\n",
       "      <td>1.994814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n",
      "Checkpoint destination directory results/marian_en_kn_finetuned/checkpoint-800 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[64109]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Starting Fine-Tuning MarianMT Kannada-English =====\n",
      "Loading samanantar dataset for kn...\n",
      "Found 5000 valid translation pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec6dbbe380a432e861353d345fc14c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f44bebd32304cec83c1fcd86a44c47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='846' max='846' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [846/846 00:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.074400</td>\n",
       "      <td>2.816344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.533400</td>\n",
       "      <td>2.703010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.164600</td>\n",
       "      <td>2.672754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.109500</td>\n",
       "      <td>2.651908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n",
      "Checkpoint destination directory results/marian_kn_en_finetuned/checkpoint-800 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[64171]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluating EN_HI Translation =====\n",
      "      Source    Reference   Prediction   BLEU  METEOR\n",
      "       Hello       नमस्ते         सलाम   0.00    0.00\n",
      "How are you? आप कैसे हैं? आप कैसे हैं? 100.00   99.22\n",
      "Good morning     सुप्रभात     सुप्रभात  17.78   50.00\n",
      "   Thank you      धन्यवाद      धन्यवाद  17.78   50.00\n",
      "   I am fine  मैं ठीक हूं  मैं ठीक हूं  56.23   98.15\n",
      "\n",
      "Average BLEU: 47.95\n",
      "Average METEOR: 74.34\n",
      "\n",
      "===== Evaluating HI_EN Translation =====\n",
      "      Source    Reference   Prediction   BLEU  METEOR\n",
      "      नमस्ते        Hello          Hi.   0.00   45.45\n",
      "आप कैसे हैं? How are you? How are you? 100.00   99.22\n",
      "    सुप्रभात Good morning Good morning  31.62   93.75\n",
      "     धन्यवाद    Thank you    Thank you  31.62   93.75\n",
      " मैं ठीक हूं    I am fine     I'm fine  13.51   33.33\n",
      "\n",
      "Average BLEU: 44.19\n",
      "Average METEOR: 80.01\n",
      "\n",
      "===== Evaluating EN_KN Translation =====\n",
      "      Source           Reference     Prediction  BLEU  METEOR\n",
      "       Hello             ನಮಸ್ಕಾರ           ಹಾಲೊ  0.00    0.00\n",
      "How are you?         ಹೇಗಿದ್ದೀರಾ?     ನೀವು ಹೇಗೆ? 11.36   23.81\n",
      "Good morning              ಶುಭೋದಯ    ಮೊದಲು ಮೊದಲು  0.00    0.00\n",
      "   Thank you          ಧನ್ಯವಾದಗಳು         ಧನ್ಯತೆ  0.00    0.00\n",
      "   I am fine ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ ನನಗೆ ಒಳ್ಳೆಯದು.  0.00    0.00\n",
      "\n",
      "Average BLEU: 11.36\n",
      "Average METEOR: 23.81\n",
      "\n",
      "===== Evaluating KN_EN Translation =====\n",
      "             Source    Reference   Prediction   BLEU  METEOR\n",
      "            ನಮಸ್ಕಾರ        Hello      Namskar   0.00    0.00\n",
      "        ಹೇಗಿದ್ದೀರಾ? How are you? How are you? 100.00   99.22\n",
      "             ಶುಭೋದಯ Good morning    Good luck  14.95   25.00\n",
      "         ಧನ್ಯವಾದಗಳು    Thank you   Thank you.  24.03   89.29\n",
      "ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ    I am fine    I am fine  56.23   98.15\n",
      "\n",
      "Average BLEU: 48.80\n",
      "Average METEOR: 77.91\n"
     ]
    }
   ],
   "source": [
    "# Train all four models\n",
    "def train_all_models():\n",
    "    \"\"\"Train all four models.\"\"\"\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    model_paths = {}\n",
    "    \n",
    "    # Train English-Hindi\n",
    "    en_hi_path = fine_tune_marian_en_hi()\n",
    "    if en_hi_path:\n",
    "        model_paths['en_hi'] = en_hi_path\n",
    "    \n",
    "    # Train Hindi-English\n",
    "    hi_en_path = fine_tune_marian_hi_en()\n",
    "    if hi_en_path:\n",
    "        model_paths['hi_en'] = hi_en_path\n",
    "    \n",
    "    # Train English-Kannada\n",
    "    en_kn_path = fine_tune_marian_en_kn()\n",
    "    if en_kn_path:\n",
    "        model_paths['en_kn'] = en_kn_path\n",
    "    \n",
    "    # Train Kannada-English\n",
    "    kn_en_path = fine_tune_marian_kn_en()\n",
    "    if kn_en_path:\n",
    "        model_paths['kn_en'] = kn_en_path\n",
    "    \n",
    "    # Evaluate models\n",
    "    if model_paths:\n",
    "        run_evaluation(model_paths)\n",
    "    \n",
    "    return model_paths\n",
    "# Capture the trained models paths\n",
    "trained_models = train_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d52adbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing translators...\n",
      "API server started on http://localhost:5005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Initialized en -> hi translator\n",
      "✓ Initialized hi -> en translator\n",
      "✓ Initialized en -> kn translator\n",
      "✓ Initialized kn -> en translator\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5005\n",
      " * Running on http://192.168.202.209:5005\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import time\n",
    "import os\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "\n",
    "# Flask API Setup\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Global Variables\n",
    "translator_cache = {}\n",
    "LANGUAGES = {'en': 'English', 'hi': 'Hindi', 'kn': 'Kannada'}\n",
    "MODEL_PATHS = {\n",
    "    'en_hi': 'results/marian_en_hi_finetuned',\n",
    "    'hi_en': 'results/marian_hi_en_finetuned',\n",
    "    'en_kn': 'results/marian_en_kn_finetuned',\n",
    "    'kn_en': 'results/marian_kn_en_finetuned'\n",
    "}\n",
    "\n",
    "# Function to get model path\n",
    "def get_model_path(src_lang, tgt_lang):\n",
    "    language_pair = f\"{src_lang}_{tgt_lang}\"\n",
    "    \n",
    "    if language_pair in MODEL_PATHS and os.path.exists(MODEL_PATHS[language_pair]):\n",
    "        return MODEL_PATHS[language_pair]\n",
    "    \n",
    "    for path in MODEL_PATHS.values():\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "        \n",
    "    return None\n",
    "\n",
    "# Function to get translator\n",
    "def get_translator(src_lang, tgt_lang):\n",
    "    cache_key = f\"{src_lang}_{tgt_lang}\"\n",
    "    \n",
    "    if cache_key in translator_cache:\n",
    "        return translator_cache[cache_key]\n",
    "    \n",
    "    model_path = get_model_path(src_lang, tgt_lang)\n",
    "    \n",
    "    if not model_path:\n",
    "        raise Exception(f\"No model available for {src_lang} -> {tgt_lang} translation\")\n",
    "    \n",
    "    try:\n",
    "        translator = UniversalTranslator(model_path)\n",
    "        translator_cache[cache_key] = translator\n",
    "        return translator\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load translator: {str(e)}\")\n",
    "\n",
    "# API Endpoints\n",
    "@app.route('/api/health', methods=['GET'])\n",
    "def health_check():\n",
    "    model_status = {}\n",
    "    # Check model paths\n",
    "    for pair, path in MODEL_PATHS.items():\n",
    "        model_status[pair] = {\n",
    "            'path': path,\n",
    "            'exists': os.path.exists(path),\n",
    "            'cached': pair in translator_cache\n",
    "        }\n",
    "    # Check translator cache\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'supported_languages': LANGUAGES,\n",
    "        'model_status': model_status,\n",
    "        'cached_translators': list(translator_cache.keys()),\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    })\n",
    "\n",
    "@app.route('/api/translate', methods=['POST'])\n",
    "def translate_text():\n",
    "    try:\n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        text = data.get('text', '').strip()\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "\n",
    "        if not text:\n",
    "            return jsonify({'error': 'No text provided for translation.'}), 400\n",
    "        if src_lang not in LANGUAGES or tgt_lang not in LANGUAGES:\n",
    "            return jsonify({'error': 'Unsupported language selected.'}), 400\n",
    "        if src_lang == tgt_lang:\n",
    "            return jsonify({'error': 'Source and target languages are the same.'}), 400\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            translator = get_translator(src_lang, tgt_lang)\n",
    "            translation = translator.translate(text, src_lang, tgt_lang)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': f'Translation failed: {str(e)}'}), 503\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        return jsonify({\n",
    "            'source_text': text,\n",
    "            'source_language': src_lang,\n",
    "            'source_language_name': LANGUAGES[src_lang],\n",
    "            'target_language': tgt_lang,\n",
    "            'target_language_name': LANGUAGES[tgt_lang],\n",
    "            'translation': translation,\n",
    "            'model_used': get_model_path(src_lang, tgt_lang),\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/batch-translate', methods=['POST'])\n",
    "def batch_translate():\n",
    "    try:\n",
    "        data = request.json\n",
    "        if not data:\n",
    "            return jsonify({'error': 'No JSON data provided.'}), 400\n",
    "            \n",
    "        texts = data.get('texts', [])\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "        \n",
    "        if not texts or not isinstance(texts, list):\n",
    "            return jsonify({'error': 'No texts array provided.'}), 400\n",
    "        if len(texts) > 100:\n",
    "            return jsonify({'error': 'Maximum 100 texts allowed per batch.'}), 400\n",
    "        if src_lang not in LANGUAGES or tgt_lang not in LANGUAGES:\n",
    "            return jsonify({'error': 'Unsupported language selected.'}), 400\n",
    "        if src_lang == tgt_lang:\n",
    "            return jsonify({'error': 'Source and target languages are the same.'}), 400\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            translator = get_translator(src_lang, tgt_lang)\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': f'Failed to load translator: {str(e)}'}), 503\n",
    "        \n",
    "        translations = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if isinstance(text, str) and text.strip():\n",
    "                try:\n",
    "                    translation = translator.translate(text.strip(), src_lang, tgt_lang)\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': translation,\n",
    "                        'success': True\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    translations.append({\n",
    "                        'source': text.strip(),\n",
    "                        'translation': None,\n",
    "                        'success': False,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "            else:\n",
    "                translations.append({\n",
    "                    'source': text,\n",
    "                    'translation': None,\n",
    "                    'success': False,\n",
    "                    'error': 'Invalid text format'\n",
    "                })\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        return jsonify({\n",
    "            'translations': translations,\n",
    "            'source_language': src_lang,\n",
    "            'target_language': tgt_lang,\n",
    "            'model_used': get_model_path(src_lang, tgt_lang),\n",
    "            'total_count': len(translations),\n",
    "            'success_count': sum(1 for t in translations if t['success']),\n",
    "            'processing_time': round(end_time - start_time, 3)\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n",
    "@app.route('/api/languages', methods=['GET'])\n",
    "def get_languages():\n",
    "    available_pairs = []\n",
    "    \n",
    "    for src in LANGUAGES.keys():\n",
    "        for tgt in LANGUAGES.keys():\n",
    "            if src != tgt:\n",
    "                model_path = get_model_path(src, tgt)\n",
    "                if model_path:\n",
    "                    available_pairs.append({\n",
    "                        'source': src,\n",
    "                        'target': tgt,\n",
    "                        'source_name': LANGUAGES[src],\n",
    "                        'target_name': LANGUAGES[tgt],\n",
    "                        'model_path': model_path\n",
    "                    })\n",
    "    \n",
    "    return jsonify({\n",
    "        'supported_languages': LANGUAGES,\n",
    "        'available_translation_pairs': available_pairs,\n",
    "        'total_pairs': len(available_pairs)\n",
    "    })\n",
    "\n",
    "def initialize_translators():\n",
    "    print(\"Initializing translators...\")\n",
    "    \n",
    "    common_pairs = [('en', 'hi'), ('hi', 'en'), ('en', 'kn'), ('kn', 'en')]\n",
    "    \n",
    "    for src_lang, tgt_lang in common_pairs:\n",
    "        try:\n",
    "            get_translator(src_lang, tgt_lang)\n",
    "            print(f\"✓ Initialized {src_lang} -> {tgt_lang} translator\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to initialize {src_lang} -> {tgt_lang}: {e}\")\n",
    "\n",
    "def start_api_server():\n",
    "    initialize_translators()\n",
    "    app.run(debug=False, use_reloader=False, host='0.0.0.0', port=5005, threaded=True)\n",
    "\n",
    "def start_translation_api():\n",
    "    thread = threading.Thread(target=start_api_server, daemon=True)\n",
    "    thread.start()\n",
    "    print(\"API server started on http://localhost:5005\")\n",
    "    return thread\n",
    "\n",
    "# Execute training and start API\n",
    "if __name__ == \"__main__\":\n",
    "    # Train all models\n",
    "    # trained_models = train_all_models()\n",
    "    \n",
    "    # Start API server\n",
    "    start_translation_api()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt_rerun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
