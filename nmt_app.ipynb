{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f79638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IMRAN\\.conda\\envs\\nmt\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import time\n",
    "# from translator import NMTTranslator\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f836e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables for the NMT application.\n",
    "LANGUAGES = {\n",
    "    'en': 'English',\n",
    "    'hi': 'Hindi',\n",
    "    'ta': 'Tamil',\n",
    "    'te': 'Telugu',\n",
    "    'bn': 'Bengali',\n",
    "    'mr': 'Marathi',\n",
    "    'gu': 'Gujarati',\n",
    "    'kn': 'Kannada',\n",
    "    'ml': 'Malayalam',\n",
    "    'pa': 'Punjabi',\n",
    "}\n",
    "\n",
    "# Translation model from Hugging Face.\n",
    "MODEL_CONFIGS = {\n",
    "    'model_name': 'facebook/mbart-large-50-many-to-many-MMT',\n",
    "\n",
    "    # Language codes for mBART model.\n",
    "    'lang_codes': {\n",
    "        'en': 'en_XX',\n",
    "        'hi': 'hi_IN',\n",
    "        'ta': 'ta_IN',\n",
    "        'te': 'te_IN',\n",
    "        'bn': 'bn_IN',\n",
    "        'mr': 'mr_IN',\n",
    "        'gu': 'gu_IN',\n",
    "        'kn': 'kn_IN',\n",
    "        'ml': 'ml_IN',\n",
    "        'pa': 'pa_IN',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e2deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NMTTranslator:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        try:\n",
    "            print(f\"Loading model: {MODEL_CONFIGS['model_name']}...\")\n",
    "            self.model = MBartForConditionalGeneration.from_pretrained(MODEL_CONFIGS['model_name'])\n",
    "            self.tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_CONFIGS['model_name'])\n",
    "            print(\"Model and tokenizer loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL: Failed to load model or tokenizer. Error: {e}\")\n",
    "\n",
    "    def translate(self, text: str, src_lang: str, tgt_lang: str):\n",
    "        if not self.model or not self.tokenizer:\n",
    "            return {\"error\": \"Translator model is not available. Check server logs for details.\"}\n",
    "\n",
    "        try:\n",
    "            src_code = MODEL_CONFIGS['lang_codes'].get(src_lang)\n",
    "            tgt_code = MODEL_CONFIGS['lang_codes'].get(tgt_lang)\n",
    "\n",
    "            if not src_code or not tgt_code:\n",
    "                return {\"error\": f\"Invalid language configuration for {src_lang} or {tgt_lang}.\"}\n",
    "\n",
    "            if tgt_code not in self.tokenizer.lang_code_to_id:\n",
    "                 return {\"error\": f\"The model does not support the target language code: {tgt_code}\"}\n",
    "\n",
    "            # Setting the source language\n",
    "            self.tokenizer.src_lang = src_code\n",
    "\n",
    "            # Encode the input text\n",
    "            encoded_text = self.tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "            # Generate the translation\n",
    "            generated_tokens = self.model.generate(\n",
    "                **encoded_text,\n",
    "                forced_bos_token_id=self.tokenizer.lang_code_to_id[tgt_code]\n",
    "            )\n",
    "\n",
    "            # Decode the generated tokens\n",
    "            translated_text = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "            return {\"translation\": translated_text}\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"An error occurred during translation: {str(e)}\"\n",
    "            print(error_message)\n",
    "            return {\"error\": \"An internal error occurred during translation.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af5552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the translator...\n",
      "Loading model: facebook/mbart-large-50-many-to-many-MMT...\n",
      "Model and tokenizer loaded successfully.\n",
      "Translator initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "# Enable Cross-Origin Resource Sharing (CORS)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize translator class\n",
    "print(\"Initializing the translator...\")\n",
    "translator = NMTTranslator()\n",
    "print(\"Translator initialized successfully.\")\n",
    "\n",
    "@app.route('/api/translate', methods=['POST'])\n",
    "def translate_text():\n",
    "    \"\"\"\n",
    "    API endpoint to handle translation requests.\n",
    "    Expects a JSON payload with 'text', 'src_lang', and 'tgt_lang'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        text = data.get('text', '').strip()\n",
    "        src_lang = data.get('src_lang', 'en')\n",
    "        tgt_lang = data.get('tgt_lang', 'hi')\n",
    "\n",
    "        # --- Input Validation ---\n",
    "        if not text:\n",
    "            return jsonify({'error': 'No text provided for translation.'}), 400\n",
    "        if src_lang not in LANGUAGES or tgt_lang not in LANGUAGES:\n",
    "            return jsonify({'error': 'Unsupported language selected.'}), 400\n",
    "        if src_lang == tgt_lang:\n",
    "            return jsonify({'error': 'Source and target languages are the same.'}), 400\n",
    "\n",
    "        # --- Perform Translation ---\n",
    "        start_time = time.time()\n",
    "        result = translator.translate(text, src_lang, tgt_lang)\n",
    "        end_time = time.time()\n",
    "\n",
    "        if \"error\" in result:\n",
    "            return jsonify(result), 500\n",
    "\n",
    "        # --- Return Successful Response ---\n",
    "        return jsonify({\n",
    "            'translation': result.get('translation'),\n",
    "            'processing_time': round(end_time - start_time, 2)\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return jsonify({'error': 'An internal server error occurred.'}), 500\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd1a4b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK 'punkt' data already downloaded.\n",
      "Downloading NLTK 'wordnet' data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\IMRAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/mbart-large-50-many-to-many-MMT...\n",
      "Model and tokenizer loaded successfully.\n",
      "Running translation evaluation...\n",
      "  - Evaluating case 1/6...\n",
      "  - Evaluating case 2/6...\n",
      "  - Evaluating case 3/6...\n",
      "  - Evaluating case 4/6...\n",
      "  - Evaluating case 5/6...\n",
      "  - Evaluating case 6/6...\n",
      "Evaluation complete.\n",
      "\n",
      "--- Evaluation Results ---\n",
      "                      Source              Reference              Prediction    BLEU  METEOR\n",
      "0        Hello, how are you?   à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?    à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?  100.00   99.77\n",
      "1               Good morning               à¤¸à¥à¤ªà¥à¤°à¤­à¤¾à¤¤                  à¤¨à¤®à¤¸à¥à¤¤à¥‡    0.00    0.00\n",
      "2        Where are you from?     à¤¤à¥à¤®à¥à¤¹à¥€ à¤•à¥à¤ à¥‚à¤¨ à¤†à¤¹à¤¾à¤¤?          à¤¤à¥‚ à¤•à¥‹à¤ à¥‚à¤¨ à¤†à¤²à¥‹à¤¸?    8.03   12.50\n",
      "3  This is a beautiful place  à¤¹à¥‡ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤ à¤¿à¤•à¤¾à¤£ à¤†à¤¹à¥‡  à¤¹à¥‡ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤¸ à¥ à¤¥à¤³ à¤†à¤¹à¥‡   17.57   72.12\n",
      "4                 à¤®à¥€ à¤ à¥€à¤• à¤†à¤¹à¥‡              I am fine                 I 'm OK   11.36   33.33\n",
      "5                     à¤¨à¤®à¤¸à¥à¤¤à¥‡                  Hello               greetings    0.00    0.00\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def download_nltk_data():\n",
    "    \"\"\"Downloads the necessary NLTK data.\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        print(\"NLTK 'punkt' data already downloaded.\")\n",
    "    except LookupError:\n",
    "        print(\"Downloading NLTK 'punkt' data...\")\n",
    "        nltk.download('punkt')  \n",
    "    \n",
    "    try:\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "        print(\"NLTK 'wordnet' data already downloaded.\")\n",
    "    except LookupError:\n",
    "        print(\"Downloading NLTK 'wordnet' data...\")\n",
    "        nltk.download('wordnet')\n",
    "        \n",
    "class TranslationEvaluator:\n",
    "    def __init__(self):\n",
    "        self.smoothing = SmoothingFunction().method1\n",
    "\n",
    "    def calculate_bleu(self, reference, candidate):\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        score = sentence_bleu([ref_tokens], cand_tokens, smoothing_function=self.smoothing)\n",
    "        return round(score * 100, 2)\n",
    "\n",
    "    # Corrected method below\n",
    "    def calculate_meteor(self, reference, candidate):\n",
    "        # Tokenize the reference and candidate sentences first\n",
    "        ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "        cand_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        \n",
    "        # Pass the tokenized lists to meteor_score\n",
    "        # Note: meteor_score expects a list of reference lists\n",
    "        score = meteor_score([ref_tokens], cand_tokens)\n",
    "        return round(score * 100, 2)\n",
    "\n",
    "def run_evaluation(translator, test_cases):\n",
    "    evaluator = TranslationEvaluator()\n",
    "    results = []\n",
    "    print(\"Running translation evaluation...\")\n",
    "    for i, case in enumerate(test_cases):\n",
    "        print(f\"  - Evaluating case {i+1}/{len(test_cases)}...\")\n",
    "        result = translator.translate(case['source'], case['src_lang'], case['tgt_lang'])\n",
    "        if 'error' in result:\n",
    "            print(f\"    Error translating '{case['source']}': {result['error']}\")\n",
    "            continue\n",
    "        pred_text = result.get('translation', '')\n",
    "        bleu = evaluator.calculate_bleu(case['reference'], pred_text)\n",
    "        meteor = evaluator.calculate_meteor(case['reference'], pred_text)\n",
    "        results.append({\n",
    "            'Source': case['source'],\n",
    "            'Reference': case['reference'],\n",
    "            'Prediction': pred_text,\n",
    "            'BLEU': bleu,\n",
    "            'METEOR': meteor\n",
    "        })\n",
    "    print(\"Evaluation complete.\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "download_nltk_data()\n",
    "nmt = NMTTranslator()\n",
    "evaluation_pairs = [\n",
    "    {'source': 'Hello, how are you?', 'reference': 'à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?', 'src_lang': 'en', 'tgt_lang': 'hi'},\n",
    "    {'source': 'Good morning', 'reference': 'à¤¸à¥à¤ªà¥à¤°à¤­à¤¾à¤¤', 'src_lang': 'en', 'tgt_lang': 'hi'},\n",
    "    {'source': 'Where are you from?', 'reference': 'à¤¤à¥à¤®à¥à¤¹à¥€ à¤•à¥à¤ à¥‚à¤¨ à¤†à¤¹à¤¾à¤¤?', 'src_lang': 'en', 'tgt_lang': 'mr'},\n",
    "    {'source': 'This is a beautiful place', 'reference': 'à¤¹à¥‡ à¤à¤• à¤¸à¥à¤‚à¤¦à¤° à¤ à¤¿à¤•à¤¾à¤£ à¤†à¤¹à¥‡', 'src_lang': 'en', 'tgt_lang': 'mr'},\n",
    "    # {    'source': 'à¤®à¥€ à¤ à¥€à¤• à¤†à¤¹à¥‡', 'reference': 'I am fine', 'src_lang': 'hi', 'tgt_lang': 'en'},\n",
    "    {'source': 'à¤®à¥€ à¤ à¥€à¤• à¤†à¤¹à¥‡', 'reference': 'I am fine', 'src_lang': 'hi', 'tgt_lang': 'en'},\n",
    "    {'source': 'à¤¨à¤®à¤¸à¥à¤¤à¥‡', 'reference': 'Hello', 'src_lang': 'hi', 'tgt_lang': 'en'}\n",
    "]\n",
    "evaluation_df = run_evaluation(nmt, evaluation_pairs)\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(evaluation_df.to_string())\n",
    "print(\"------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f61b08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# 1. SETUP: Define model, languages, and dataset parameters\n",
    "# -----------------------------------------------------------------\n",
    "# model_name = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "model_name = \"google/mt5-base\"\n",
    "# Important: Use the correct language codes for IndicTrans2\n",
    "src_lang = \"eng_Latn\"  # English\n",
    "tgt_lang = \"hin_Deva\"  # Hindi\n",
    "dataset_name = \"ai4bharat/samanantar\"\n",
    "dataset_config = \"hi\" # English-Hindi pair\n",
    "output_dir = \"results/indictrans2-finetuned-en-hi\"\n",
    "\n",
    "# For demonstration, we'll use a small subset of the data.\n",
    "# Increase these numbers for a real fine-tuning job.\n",
    "train_sample_size = 2000\n",
    "valid_sample_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ab4257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library version: 4.55.2\n",
      "\n",
      "'Seq2SeqTrainingArguments' is being loaded from this file:\n",
      "c:\\Users\\IMRAN\\.conda\\envs\\nmt\\lib\\site-packages\\transformers\\training_args_seq2seq.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Transformers library version: {transformers.__version__}\")\n",
    "\n",
    "# Get the module object from which the class was imported\n",
    "module_name = Seq2SeqTrainingArguments.__module__\n",
    "module_object = sys.modules[module_name]\n",
    "\n",
    "print(\"\\n'Seq2SeqTrainingArguments' is being loaded from this file:\")\n",
    "print(module_object.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b96d285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'ai4bharat/samanantar' dataset (hi configuration)...\n",
      "Creating a validation split from the training data...\n",
      "Loaded 2000 training samples and 200 validation samples.\n"
     ]
    }
   ],
   "source": [
    "# 2. DATA LOADING: Fetch the dataset and create a validation split\n",
    "# -----------------------------------------------------------------\n",
    "print(f\"Loading '{dataset_name}' dataset ({dataset_config} configuration)...\")\n",
    "dataset_config = \"hi\"\n",
    "dataset = load_dataset(dataset_name, dataset_config)\n",
    "\n",
    "# Since there's no 'validation' split, we create one from the 'train' split.\n",
    "# We'll use 10% of the training data for validation.\n",
    "print(\"Creating a validation split from the training data...\")\n",
    "train_validation_split = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# The split creates a new DatasetDict with 'train' and 'test' keys.\n",
    "# We will use its 'train' for training and its 'test' as our validation set.\n",
    "full_train_dataset = train_validation_split['train']\n",
    "full_valid_dataset = train_validation_split['test']\n",
    "\n",
    "# Now, select the smaller samples for our quick fine-tuning run\n",
    "# We add a check to ensure we don't request more samples than available\n",
    "train_sample_size = min(train_sample_size, len(full_train_dataset))\n",
    "valid_sample_size = min(valid_sample_size, len(full_valid_dataset))\n",
    "\n",
    "train_dataset = full_train_dataset.select(range(train_sample_size))\n",
    "valid_dataset = full_valid_dataset.select(range(valid_sample_size))\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training samples and {len(valid_dataset)} validation samples.\")\n",
    "# Example: print(train_dataset[0]) -> {'translation': {'en': '...', 'hi': '...'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77bcfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model for 'google/mt5-base'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\IMRAN\\.conda\\envs\\nmt\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. TOKENIZER & MODEL: Load the pre-trained model and tokenizer\n",
    "# -----------------------------------------------------------------\n",
    "print(f\"Loading tokenizer and model for '{model_name}'...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Set the source and target languages for the tokenizer\n",
    "tokenizer.src_lang = src_lang\n",
    "tokenizer.tgt_lang = tgt_lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0506411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    }
   ],
   "source": [
    "# 4. PREPROCESSING: Create a function to tokenize the data\n",
    "# -----------------------------------------------------------------\n",
    "def preprocess_function(examples):\n",
    "    # The dataset has 'src' for source (English) and 'tgt' for target (Hindi)\n",
    "    inputs = examples[\"src\"]\n",
    "    targets = examples[\"tgt\"]\n",
    "\n",
    "    # THIS IS THE FIX: Prepend the language codes to each input sentence\n",
    "    # The IndicTrans2 tokenizer expects this specific format for this model.\n",
    "    # prefixed_inputs = [f\"{src_lang} {tgt_lang} {text}\" for text in inputs]\n",
    "\n",
    "    # The tokenizer will now correctly handle the prefixed inputs\n",
    "    model_inputs = tokenizer(\n",
    "        # prefixed_inputs,  # Use the prefixed inputs for indictrans2\n",
    "        inputs,  # Use the original inputs for google/mt5-base\n",
    "        text_target=targets,\n",
    "        max_length=128,\n",
    "        truncation=True\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Preprocessing datasets...\")\n",
    "# No changes needed here, the fix is entirely within the function above\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"idx\", \"src\", \"tgt\"])\n",
    "tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True, remove_columns=[\"idx\", \"src\", \"tgt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "839c80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall -y transformers\n",
    "# %pip cache purge\n",
    "# %pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13388235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library version: 4.55.2\n",
      "\n",
      "'Seq2SeqTrainingArguments' is being loaded from this file:\n",
      "c:\\Users\\IMRAN\\.conda\\envs\\nmt\\lib\\site-packages\\transformers\\training_args_seq2seq.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Transformers library version: {transformers.__version__}\")\n",
    "\n",
    "# Get the module object from which the class was imported\n",
    "module_name = Seq2SeqTrainingArguments.__module__\n",
    "module_object = sys.modules[module_name]\n",
    "\n",
    "print(\"\\n'Seq2SeqTrainingArguments' is being loaded from this file:\")\n",
    "print(module_object.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3549a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 training enabled: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IMRAN\\AppData\\Local\\Temp\\ipykernel_28596\\1521773053.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# 5. TRAINING SETUP: Configure the training arguments and trainer\n",
    "# -----------------------------------------------------------------\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Check if a GPU is available and set fp16 accordingly\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "print(f\"FP16 training enabled: {use_fp16}\")\n",
    "\n",
    "# This code is CORRECT for modern versions of the transformers library\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    # evaluation_strategy=\"epoch\",  # This is not recognized\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    # optim=\"paged_adamw_8bit\",  # only for CUDA based training\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=use_fp16,\n",
    "    push_to_hub=False,\n",
    "    no_cuda=True,      # Forcing to run on CPU\n",
    "    fp16=False,        # For CPU based training\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55bcc1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33e6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/375 00:41 < 4:14:59, 0.02 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 6. START TRAINING: Fine-tune the model\n",
    "# -----------------------------------------------------------------\n",
    "print(\"ðŸš€ Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Fine-tuning complete. Saving model to '{output_dir}'\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"âœ… Model and tokenizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Run the Flask app in debug mode\n",
    "    app.run(debug=True, port=3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
